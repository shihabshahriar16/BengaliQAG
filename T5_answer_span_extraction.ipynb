{"cells":[{"cell_type":"markdown","source":["<h1>Answer Span Extraction</h1>\n","\n"],"metadata":{"id":"SWwy1S9ZXl4F"}},{"cell_type":"code","source":["!pip install transformers==\"4.25.1\" sentencepiece==\"0.1.97\" utoken==\"0.1.8\" nltk==\"3.8.1\" datasets==\"2.8.0\" torch==\"1.13.1+cu116\" numpy==\"1.21.6\" seqeval==\"1.2.2\" pytorch_lightning==\"1.9.0\" tqdm==\"4.64.1\" --quiet"],"metadata":{"id":"scOS5s0_ZCdf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2epab9lQtqw"},"outputs":[],"source":["# import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WqcwP916Dwq"},"outputs":[],"source":["import random\n","import numpy as np\n","import torch\n","def set_seed(seed):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(seed)\n","\n","set_seed(42)"]},{"cell_type":"markdown","metadata":{"id":"_BlIYFZKgmOU"},"source":["# Span Selection with T5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cBQiMj-p5lfz","outputId":"e9552761-2f47-4b07-893f-ed1828da5c9c"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-01-10 11:19:24.710677: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-01-10 11:19:25.004054: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-01-10 11:19:25.079914: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-01-10 11:19:26.001669: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64\n","2023-01-10 11:19:26.001920: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64\n","2023-01-10 11:19:26.001935: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"]}],"source":["import argparse\n","import glob\n","import os\n","import json\n","import time\n","import logging\n","import random\n","import re\n","from itertools import chain\n","from string import punctuation\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pytorch_lightning as pl\n","\n","\n","from transformers import (\n","    AdamW,\n","    MT5ForConditionalGeneration,\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    AutoTokenizer,\n","    get_linear_schedule_with_warmup\n",")"]},{"cell_type":"markdown","metadata":{"id":"8h8R2bLDhjlc"},"source":["# Model\n","\n","Majority of the code here is adapted from [here](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb) which uses the pytorch-lightning framework for training neural networks. T5 has shown that it can generate state of the art on many tasks as long as it can be cast as a text-to-text problem"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KL8_p4YS6H0a"},"outputs":[],"source":["class T5FineTuner(pl.LightningModule):\n","    def __init__(self, hparam):\n","        super(T5FineTuner, self).__init__()\n","        self.hparam = hparam\n","\n","        self.model = T5ForConditionalGeneration.from_pretrained(\n","            hparam.model_name_or_path)\n","        self.tokenizer = AutoTokenizer.from_pretrained(\n","            hparam.model_name_or_path\n","        )\n","        self.save_hyperparameters()\n","\n","    def is_logger(self):\n","        return True\n","\n","    def forward(\n","        self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n","    ):\n","        return self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            labels=lm_labels,\n","        )\n","\n","    def _step(self, batch):\n","        lm_labels = batch[\"target_ids\"]\n","        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n","\n","        outputs = self(\n","            input_ids=batch[\"source_ids\"],\n","            attention_mask=batch[\"source_mask\"],\n","            lm_labels=lm_labels,\n","            decoder_attention_mask=batch['target_mask']\n","        )\n","\n","        loss = outputs[0]\n","\n","        return loss\n","\n","    def training_step(self, batch, batch_idx):\n","        loss = self._step(batch)\n","\n","        tensorboard_logs = {\"train_loss\": loss}\n","        return {\"loss\": loss, \"log\": tensorboard_logs}\n","\n","    def training_epoch_end(self, outputs):\n","        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n","        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n","\n","    def validation_step(self, batch, batch_idx):\n","        loss = self._step(batch)\n","        return {\"val_loss\": loss}\n","\n","    def validation_epoch_end(self, outputs):\n","        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n","        tensorboard_logs = {\"val_loss\": avg_loss}\n","\n","    def configure_optimizers(self):\n","        \"Prepare optimizer and schedule (linear warmup and decay)\"\n","\n","        model = self.model\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.hparam.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters,\n","                          lr=self.hparam.learning_rate, eps=self.hparam.adam_epsilon)\n","        self.opt = optimizer\n","        return [optimizer]\n","\n","    def optimizer_step(self,\n","                       epoch=None,\n","                       batch_idx=None,\n","                       optimizer=None,\n","                       optimizer_idx=None,\n","                       optimizer_closure=None,\n","                       on_tpu=None,\n","                       using_native_amp=None,\n","                       using_lbfgs=None\n","                       ):\n","\n","        optimizer.step(closure=optimizer_closure)\n","        optimizer.zero_grad()\n","        self.lr_scheduler.step()\n","\n","    def get_tqdm_dict(self):\n","        tqdm_dict = {\"loss\": \"{:.3f}\".format(\n","            self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n","\n","        return tqdm_dict\n","\n","    def train_dataloader(self):\n","        train_dataset = get_dataset(\n","            tokenizer=self.tokenizer, type_path=\"train\", args=self.hparam)\n","        dataloader = DataLoader(train_dataset, batch_size=self.hparam.train_batch_size,\n","                                drop_last=True, shuffle=True, num_workers=2)\n","        t_total = (\n","            (len(dataloader.dataset) //\n","             (self.hparam.train_batch_size * max(1, self.hparam.n_gpu)))\n","            // self.hparam.gradient_accumulation_steps\n","            * float(self.hparam.num_train_epochs)\n","        )\n","        scheduler = get_linear_schedule_with_warmup(\n","            self.opt, num_warmup_steps=self.hparam.warmup_steps, num_training_steps=t_total\n","        )\n","        self.lr_scheduler = scheduler\n","        return dataloader\n","\n","    def val_dataloader(self):\n","        val_dataset = get_dataset(\n","            tokenizer=self.tokenizer, type_path=\"validation\", args=self.hparam)\n","        return DataLoader(val_dataset, batch_size=self.hparam.eval_batch_size, num_workers=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6VQpUMNe6Wf8"},"outputs":[],"source":["logger = logging.getLogger(__name__)\n","\n","class LoggingCallback(pl.Callback):\n","  def on_validation_end(self, trainer, pl_module):\n","    logger.info(\"***** Validation results *****\")\n","    if pl_module.is_logger():\n","      metrics = trainer.callback_metrics\n","      # Log results\n","      for key in sorted(metrics):\n","        if key not in [\"log\", \"progress_bar\"]:\n","          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","\n","  def on_test_end(self, trainer, pl_module):\n","    logger.info(\"***** Test results *****\")\n","\n","    if pl_module.is_logger():\n","      metrics = trainer.callback_metrics\n","\n","      # Log and save results to file\n","      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n","      with open(output_test_results_file, \"w\") as writer:\n","        for key in sorted(metrics):\n","          if key not in [\"log\", \"progress_bar\"]:\n","            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I55QMghp6YbE"},"outputs":[],"source":["args_dict = dict(\n","    data_dir=\"./\", # path for data files\n","    output_dir=\"./tmp\", # path to save the checkpoints\n","    model_name_or_path='csebuetnlp/banglat5',\n","    tokenizer_name_or_path='csebuetnlp/banglat5',\n","    max_seq_length=256,\n","    learning_rate=3e-5,\n","    weight_decay=0.0,\n","    adam_epsilon=1e-8,\n","    warmup_steps=0,\n","    train_batch_size=8,\n","    eval_batch_size=8,\n","    num_train_epochs=3,\n","    gradient_accumulation_steps=1,\n","    n_gpu=1,\n","    early_stop_callback=False,\n","    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n","    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n","    max_grad_norm=1, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n","    seed=42,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aS-G83jnQtq1"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"soCZS7n07Ts1"},"outputs":[],"source":["import json\n","from pathlib import Path\n","import torch\n","from torch.utils.data import DataLoader\n","import time\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LgZKb7T48Mzw"},"outputs":[],"source":["class SquadDataset(Dataset):\n","  def __init__(self, tokenizer, dataset, type_path, max_len_context=128,max_len_ans=30):\n","\n","    self.data = dataset[type_path]\n","    self.max_len_context = max_len_context\n","    self.max_len_ans = max_len_ans\n","    self.tokenizer = tokenizer\n","    self.inputs = []\n","    self.targets = []\n","\n","    self._build()\n","  \n","  def __len__(self):\n","    return len(self.inputs)\n","  \n","  def __getitem__(self, index):\n","    source_ids = self.inputs[index][\"input_ids\"].squeeze()\n","    target_ids = self.targets[index][\"input_ids\"].squeeze()\n","\n","    src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n","    target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n","\n","    return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n","  \n","  def _build(self):\n","    for idx in range(len(self.data)):\n","      input_, target = self.data[idx][\"context\"], self.data[idx][\"answer\"]    \n","      \n","      input_ = input_.lower()\n","      target = target.lower()\n","\n","       # tokenize inputs\n","      tokenized_inputs = self.tokenizer.batch_encode_plus(\n","          [input_], max_length=self.max_len_context, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n","      )\n","       # tokenize targets\n","      tokenized_targets = self.tokenizer.batch_encode_plus(\n","          [target],max_length=self.max_len_ans, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n","      )\n","\n","      self.inputs.append(tokenized_inputs)\n","      self.targets.append(tokenized_targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mRvhyY7FB4FI"},"outputs":[],"source":["path = Path('./squad1_data/squad1_translated_final_aligned.json')\n","\n","# Open .json file\n","with open(path, 'rb') as f:\n","    squad_dict = json.load(f)\n","\n","train = {}\n","train['data'] = []\n","\n","# Search for each passage, its question and its answer\n","for gi in range(0,400):\n","    group = squad_dict['data'][gi]\n","    for passage in group['paragraphs']:\n","        context_list = passage['bangla_context_list']\n","        for qa in passage['qas']:\n","            data = {}\n","            qid = qa['id']\n","            data['id'] = qid\n","            answer = qa['answers'][0]\n","            context = context_list[answer['index_c_tran_with_ans']]\n","            data['context'] = context\n","            # answer_start = []\n","            # answer_text = []\n","            # for answer in qa['answers']:\n","            if answer['align_score'] >= 0.5 and answer['a_tran'] in context and len(answer['a_tran'])!=0:\n","                data['answer'] = answer['a_tran']\n","                train['data'].append(data)\n","\n","\n","out_file = open('./train_ase_sq1.json', \"w\")\n","json.dump(train, out_file, indent = 4) # save whole data replace parts later\n","out_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvCr21nYB6zv"},"outputs":[],"source":["val = {}\n","val['data'] = []\n","\n","# Search for each passage, its question and its answer\n","for gi in range(400,len(squad_dict['data'])):\n","    group = squad_dict['data'][gi]\n","    for passage in group['paragraphs']:\n","        context_list = passage['bangla_context_list']\n","        for qa in passage['qas']:\n","            data = {}\n","            qid = qa['id']\n","            data['id'] = qid\n","            answer = qa['answers'][0]\n","            context = context_list[answer['index_c_tran_with_ans']]\n","            data['context'] = context\n","            # answer_start = []\n","            # answer_text = []\n","            # for answer in qa['answers']:\n","            if answer['align_score'] >= 0.5 and answer['a_tran'] in context and len(answer['a_tran'])!=0:\n","                data['answer'] = answer['a_tran']\n","                val['data'].append(data)\n","\n","out_file = open('./val_ase_sq1.json', \"w\")\n","json.dump(val, out_file, indent = 4) # save whole data replace parts later\n","out_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fzc6NIbaB88n"},"outputs":[],"source":["path = Path('./squad1_data/squad1_dev_translated_final_aligned.json')\n","\n","# Open .json file\n","with open(path, 'rb') as f:\n","    squad_dict = json.load(f)\n","\n","test = {}\n","test['data'] = []\n","\n","# Search for each passage, its question and its answer\n","for gi in range(len(squad_dict['data'])):\n","    group = squad_dict['data'][gi]\n","    for passage in group['paragraphs']:\n","        context_list = passage['bangla_context_list']\n","        for qa in passage['qas']:\n","            data = {}\n","            qid = qa['id']\n","            data['id'] = qid\n","            answer = qa['answers'][0]\n","            context = context_list[answer['index_c_tran_with_ans']]\n","            data['context'] = context\n","            # answer_start = []\n","            # answer_text = []\n","            # for answer in qa['answers']:\n","            if answer['align_score'] >= 0.5 and answer['a_tran'] in context and len(answer['a_tran'])!=0:\n","                data['answer'] = answer['a_tran']\n","                test['data'].append(data)\n","\n","out_file = open('./test_ase_sq1.json', \"w\")\n","json.dump(test, out_file, indent = 4) # save whole data replace parts later\n","out_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["80207963d1aa4a9ca79470640b1e001d","ec1403469a244be0a0209ee087af1d09","a55b7351b9584ba4a3c06ebc1b7c1282","dee01aa3f00e4d9c921243708de945f6","0b15ac43bf484912bd947056267d6195","87488c6a7e1c49028c37cc11577bb3a5","4fdca4355f644d64ae5ba1b8d689aa14","54420e4ee2fb4699bba6af5f8ea2d5e6","fb3652e583a843bf82502348cdaa4e18","ff4eb2db09c348479d39d8b500e4244a","74f06e8d81ee4c16b0c1e93f5926280b","7983bdbc2c9d4b2cae8018a92c7eac3a"]},"id":"pX26R5NnCG_w","outputId":"71084353-8e3c-4cf6-b9a7-655e508e4cb7"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using custom data configuration default-77746c7d4018536c\n","Found cached dataset json (/home/dlpc01/.cache/huggingface/datasets/json/default-77746c7d4018536c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7983bdbc2c9d4b2cae8018a92c7eac3a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset, load_metric\n","data_files = {}\n","\n","data_files[\"train\"] = \"./train_ase_sq1.json\"\n","extension = data_files[\"train\"].split(\".\")[-1]\n","\n","data_files[\"validation\"] = \"./val_ase_sq1.json\"\n","extension = data_files[\"validation\"].split(\".\")[-1]\n","\n","data_files[\"test\"] = \"./test_ase_sq1.json\"\n","extension = data_files[\"train\"].split(\".\")[-1]\n","\n","raw_datasets = load_dataset(\n","    extension,\n","    data_files=data_files,\n","    field=\"data\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sFazhLYC7UZ"},"outputs":[],"source":["dataset = raw_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DOZex9tlC_mP"},"outputs":[],"source":["dataset['train'][0][\"answer\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7eSAir8g80rm"},"outputs":[],"source":["print(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XrlJEayI-4tS"},"outputs":[],"source":["input_dataset = SquadDataset(tokenizer=tokenizer, dataset=dataset, type_path='train')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dVh9LJueQtq4"},"outputs":[],"source":["test_dataset = SquadDataset(tokenizer=tokenizer, dataset=dataset, type_path='test')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eTQxeKTZQtq4"},"outputs":[],"source":["dataset['test'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJ02SXgv_UW8"},"outputs":[],"source":["data = input_dataset[0]\n","\n","print(tokenizer.decode(data[\"source_ids\"], skip_special_tokens=False))\n","print(tokenizer.decode(data[\"target_ids\"], skip_special_tokens=False))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sepl17_eCHy4"},"outputs":[],"source":["!mkdir -p t5_ner"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Toa_qnXDrTw"},"outputs":[],"source":["args = argparse.Namespace(**args_dict)\n","model = T5FineTuner(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dIZ3LwE3DXNo"},"outputs":[],"source":["# checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","#     filename=args.output_dir+\"/checkpoint.pth\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n","# )\n","\n","train_params = dict(\n","    accumulate_grad_batches=args.gradient_accumulation_steps,\n","    gpus=args.n_gpu,\n","    max_epochs=args.num_train_epochs,\n","    #early_stop_callback=False,\n","    precision= 16 if args.fp_16 else 32,\n","    #amp_level=args.opt_level,\n","    gradient_clip_val=args.max_grad_norm,\n","    callbacks=[LoggingCallback()],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MxBEgT6MDqe3"},"outputs":[],"source":["def get_dataset(tokenizer, type_path, args):\n","    tokenizer.max_length = args.max_seq_length\n","    tokenizer.model_max_length = args.max_seq_length\n","    data_files = {}\n","\n","    data_files[\"train\"] = \"./train_ase_sq1.json\"\n","    extension = data_files[\"train\"].split(\".\")[-1]\n","\n","    data_files[\"validation\"] = \"./val_ase_sq1.json\"\n","    extension = data_files[\"validation\"].split(\".\")[-1]\n","\n","    data_files[\"test\"] = \"./test_ase_sq1.json\"\n","    extension = data_files[\"train\"].split(\".\")[-1]\n","\n","    dataset = load_dataset(\n","        extension,\n","        data_files=data_files,\n","        field=\"data\"\n","    )\n","    \n","    return SquadDataset(tokenizer=tokenizer, dataset=dataset, type_path=type_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KCKmlJ5DDaHw"},"outputs":[],"source":["trainer = pl.Trainer(**train_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sxQ-s0izFQGQ"},"outputs":[],"source":["trainer.fit(model)"]},{"cell_type":"markdown","metadata":{"id":"Nan0hu-nj5Zm"},"source":["## Load the Stored Model and Evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27yNiUp_W1pn"},"outputs":[],"source":["model = model.load_from_checkpoint(\"./lightning_logs/version_3/checkpoints/epoch=2-step=26811.ckpt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SetP-RAyQtq6"},"outputs":[],"source":["def sentence_tokenizer(text: str) -> list:\n","    terminator = [\"৷\",\"|\",\"।\", \"?\", \"!\"]\n","    tokens = []\n","    for i in text:\n","        if i in terminator:\n","            my_string = text[:text.index(i)+1]\n","            text = text[text.index(i)+1:]\n","            tokens.append(my_string.strip())\n","    if len(tokens)==0:\n","        return [text]\n","    return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUkTc9J0Qtq5"},"outputs":[],"source":["f = open(\"./BARDContexts.json\")\n","text_data = json.load(f)\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQ-XKUSbQtq5"},"outputs":[],"source":["from tqdm.notebook import tqdm\n","\n","device = \"cuda\"\n","model.model.eval()\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["6947d8a0b5294eadb53e4a93ea6fc701"]},"id":"2wlgi1rGQtq5","outputId":"232977c8-a3ea-476b-cb75-d14dc423bbc7"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6947d8a0b5294eadb53e4a93ea6fc701","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/20000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["for data in tqdm(text_data['data']):\n","    context_list = sentence_tokenizer(data['context'])\n","    tokenized_inputs = tokenizer(context_list, max_length=256, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n","    \n","    tokenized_inputs = tokenized_inputs.to(device)\n","    outs = model.model.generate(input_ids=tokenized_inputs['input_ids'],\n","                            attention_mask=tokenized_inputs['attention_mask'])\n","    dec = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip() for ids in outs]\n","    \n","    data['sent_list'] = context_list\n","    data['answers'] = []\n","    for i in range(len(context_list)):\n","        if dec[i] in context_list[i]:\n","            data['answers'].append({'text':dec[i],'sent_with_ans':i})\n","                                      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m44XxtRVQtq5"},"outputs":[],"source":["final_out_file = open('./BARDContextandAnswer.json', \"w\")\n","json.dump(text_data, final_out_file, indent = 4) # save whole data replace parts later\n","final_out_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jfZOrbyRQtq5","outputId":"e7c19543-2855-4026-ede8-c5f935c2d129"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'title': 'state', 'context': 'চাঁপাইনবাবগঞ্জের শিবগঞ্জ উপজেলার কানসাট গোপালনগর এলাকায় ভাড়াবাসা থেকে গতকাল সোমবার ভারতীয় এক লাখ জাল রুপিসহ এক ব্যক্তিকে গ্রেপ্তার করেছে র্যাব-৫। তাঁর নাম মো. মাইদুর (৩৩)।মাইদুর খড়কপুর নিমতলা এলাকার বাসিন্দা। ব্যবসা সূত্রে তিনি গোলাপনগর এলাকায় থাকতেন।চাঁপাইনবাবগঞ্জ র্যাব-৫ ক্যাম্পের সহকারী পুলিশ সুপার (এএসপি) নুরে আলম বলেন, গোপন সংবাদের ভিত্তিতে দুপুর সাড়ে ১২টার দিকে গোপালনগর মোড়ের রবিউল ইসলামের ভাড়াবাড়িতে অভিযান চালিয়ে মাইদুরকে গ্রেপ্তার করা হয়। এ ঘটনায় শিবগঞ্জ থানায় মামলা হয়েছে।', 'sent_list': ['চাঁপাইনবাবগঞ্জের শিবগঞ্জ উপজেলার কানসাট গোপালনগর এলাকায় ভাড়াবাসা থেকে গতকাল সোমবার ভারতীয় এক লাখ জাল রুপিসহ এক ব্যক্তিকে গ্রেপ্তার করেছে র্যাব-৫।', 'তাঁর নাম মো. মাইদুর (৩৩)।', 'মাইদুর খড়কপুর নিমতলা এলাকার বাসিন্দা।', 'ব্যবসা সূত্রে তিনি গোলাপনগর এলাকায় থাকতেন।', 'চাঁপাইনবাবগঞ্জ র্যাব-৫ ক্যাম্পের সহকারী পুলিশ সুপার (এএসপি) নুরে আলম বলেন, গোপন সংবাদের ভিত্তিতে দুপুর সাড়ে ১২টার দিকে গোপালনগর মোড়ের রবিউল ইসলামের ভাড়াবাড়িতে অভিযান চালিয়ে মাইদুরকে গ্রেপ্তার করা হয়।', 'এ ঘটনায় শিবগঞ্জ থানায় মামলা হয়েছে।'], 'answers': [{'text': 'এক লাখ জাল রুপিসহ এক ব্যক্তিকে', 'sent_with_ans': 0}, {'text': 'মো. মাইদুর (৩৩)', 'sent_with_ans': 1}, {'text': 'গোলাপনগর', 'sent_with_ans': 3}, {'text': 'রবিউল ইসলামের', 'sent_with_ans': 4}]}\n"]}],"source":["print(text_data['data'][49])"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0b15ac43bf484912bd947056267d6195":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fdca4355f644d64ae5ba1b8d689aa14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54420e4ee2fb4699bba6af5f8ea2d5e6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74f06e8d81ee4c16b0c1e93f5926280b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80207963d1aa4a9ca79470640b1e001d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ec1403469a244be0a0209ee087af1d09","IPY_MODEL_a55b7351b9584ba4a3c06ebc1b7c1282","IPY_MODEL_dee01aa3f00e4d9c921243708de945f6"],"layout":"IPY_MODEL_0b15ac43bf484912bd947056267d6195"}},"87488c6a7e1c49028c37cc11577bb3a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a55b7351b9584ba4a3c06ebc1b7c1282":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_54420e4ee2fb4699bba6af5f8ea2d5e6","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb3652e583a843bf82502348cdaa4e18","value":3}},"dee01aa3f00e4d9c921243708de945f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff4eb2db09c348479d39d8b500e4244a","placeholder":"​","style":"IPY_MODEL_74f06e8d81ee4c16b0c1e93f5926280b","value":" 3/3 [00:00&lt;00:00,  5.83it/s]"}},"ec1403469a244be0a0209ee087af1d09":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87488c6a7e1c49028c37cc11577bb3a5","placeholder":"​","style":"IPY_MODEL_4fdca4355f644d64ae5ba1b8d689aa14","value":"100%"}},"fb3652e583a843bf82502348cdaa4e18":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ff4eb2db09c348479d39d8b500e4244a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
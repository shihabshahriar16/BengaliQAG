{"cells":[{"cell_type":"markdown","source":["<h1> Question Generation </h1>\n","\n","\n","*   This notebook requires SQuAD dataset or a QA dataset fomratted as squad dataset.\n","*   We will use the translations of SQuAD1.1 or SQuAD2.0 that we did using the notebooks \"NLLB translation squad 1.ipynb\" or \"NLLB translation squad 2.ipynb\"\n","*   The final translations using notebooks \"NLLB translation squad 1.ipynb\" and \"NLLB translation squad 2.ipynb\" are \"squad1_translated_final_aligned.json\" and \"squad2_translated_final_aligned.json\"\n","\n"],"metadata":{"id":"ZumfEAsgHnk3"}},{"cell_type":"code","source":["!pip install transformers==\"4.25.1\" sentencepiece==\"0.1.97\" utoken==\"0.1.8\" nltk==\"3.8.1\" datasets==\"2.8.0\" torch==\"1.13.1+cu116\" numpy==\"1.21.6\" tqdm==\"4.64.1\" --quiet"],"metadata":{"id":"MAvuYa7yeKfs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""],"metadata":{"id":"ZF0MMnWAYOOJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","import numpy as np\n","import torch\n","def set_seed(seed):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(seed)\n","\n","set_seed(42)"],"metadata":{"id":"OX8bxPg_eByc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v5Jlcrg0ghs-"},"outputs":[],"source":["import json\n","from pathlib import Path\n","import torch\n","from torch.utils.data import DataLoader\n","import time"]},{"cell_type":"markdown","metadata":{"id":"X55oFpqzd78h"},"source":["## ***Step 1:*** Format and split the translated QA dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U7XcbS3afYOf"},"outputs":[],"source":["import re\n","import string\n","from collections import Counter\n","from utoken import utokenize\n","from utoken import detokenize\n","tok = utokenize.Tokenizer(lang_code='ben')\n","detok = detokenize.Detokenizer(lang_code='ben')\n","\n","def repeating_prob(context):\n","    context_tok = tok.utokenize_string(context.translate(str.maketrans('', '', string.punctuation))).split(\" \")\n","    tok_count =  [context_tok.count(i) for i in context_tok]\n","    rep_token =  context_tok[tok_count.index(max(tok_count))]\n","    indices_obj = re.finditer(rep_token,context)\n","    indices = [index.start() for index in indices_obj]\n","    indices_diff = [t - s for s, t in zip(indices, indices[1:])]\n","    if len(indices_diff)==0:\n","        return 0\n","    indices_diff_count = [indices_diff.count(i) for i in indices_diff]\n","    return max(indices_diff_count)\n","\n","\n","def repeating_char_prob(context):\n","    context_tok = tok.utokenize_string(context.translate(str.maketrans('', '', string.punctuation))).split(\" \")\n","    max_char_in_each_tok = [Counter(tok).most_common(1)[0][1] for tok in context_tok]\n","    return max(max_char_in_each_tok)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6b2EMVb-fhwA"},"outputs":[],"source":["path = Path('./squad1_translated_final_aligned.json')\n","\n","# Open .json file\n","with open(path, 'rb') as f:\n","    squad_dict = json.load(f)\n","\n","texts = []\n","queries = []\n","answers = []\n","\n","# Search for each passage, its question and its answer\n","for gi in range(400):\n","    group = squad_dict['data'][gi]\n","    for passage in group['paragraphs']:\n","        context_list = passage['bangla_context_list']\n","        for qa in passage['qas']:\n","            question = qa['q_tran'].strip() # question\n","            if repeating_prob(question)>3 or repeating_char_prob(question)>10:\n","                continue\n","            context = context_list[qa['answers'][0]['index_c_tran_with_ans']]\n","            if repeating_prob(context)>3 or repeating_char_prob(context)>10:\n","                continue\n","            if qa['answers'][0]['align_score'] >= 0.5 and qa['answers'][0]['a_tran'] in context:\n","                texts.append(context)\n","                queries.append(question)\n","                answers.append(qa['answers'][0]['a_tran'])\n","\n","\n","train_texts, train_queries, train_answers = texts, queries, answers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M62WAIpmHP2l"},"outputs":[],"source":["path = Path('./squad1_translated_final_aligned.json')\n","\n","# Open .json file\n","with open(path, 'rb') as f:\n","    squad_dict = json.load(f)\n","\n","texts = []\n","queries = []\n","answers = []\n","\n","# Search for each passage, its question and its answer\n","for gi in range(400,442):\n","    group = squad_dict['data'][gi]\n","    for passage in group['paragraphs']:\n","        context_list = passage['bangla_context_list']\n","        for qa in passage['qas']:\n","            question = qa['q_tran'].strip() # question\n","            if repeating_prob(question)>3 or repeating_char_prob(question)>10:\n","                continue\n","            context = context_list[qa['answers'][0]['index_c_tran_with_ans']]\n","            if repeating_prob(context)>3 or repeating_char_prob(context)>10:\n","                continue\n","            if qa['answers'][0]['align_score'] >= 0.5 and qa['answers'][0]['a_tran'] in context:\n","                texts.append(context)\n","                queries.append(question)\n","                answers.append(qa['answers'][0]['a_tran'])\n","\n","\n","val_texts, val_queries, val_answers = texts, queries, answers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_JPP5Daci2V8"},"outputs":[],"source":["path = Path('./squad1_dev_translated_final_aligned.json') # index_c_tran_with_ans\n","\n","# Open .json file\n","with open(path, 'rb') as f:\n","    squad_dict = json.load(f)\n","\n","q_ids = []\n","texts = []\n","queries = []\n","answers = []\n","\n","# Search for each passage, its question and its answer\n","for gi in range(len(squad_dict['data'])):\n","    group = squad_dict['data'][gi]\n","    for passage in group['paragraphs']:\n","        context_list = passage['bangla_context_list']\n","        for qa in passage['qas']:\n","            qid = qa['id']\n","            question = qa['q_tran'].strip() # question\n","            if repeating_prob(question)>3 or repeating_char_prob(question)>10:\n","                continue\n","            context = context_list[qa['answers'][0]['index_c_tran_with_ans']]\n","            if repeating_prob(context)>3 or repeating_char_prob(context)>10:\n","                continue\n","            if qa['answers'][0]['align_score'] >= 0.5 and qa['answers'][0]['a_tran'] in context:\n","                texts.append(context)\n","                queries.append(question)\n","                answers.append(qa['answers'][0]['a_tran'])\n","                q_ids.append(qid)\n","\n","\n","test_texts, test_queries, test_answers = texts, queries, answers"]},{"cell_type":"markdown","metadata":{"id":"PEN89d8Mjlw3"},"source":["## ***Step 2:*** Check the data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w3f9hbvMjJ1b","outputId":"fb14845a-b265-47b1-c352-bec7278be244"},"outputs":[{"name":"stdout","output_type":"stream","text":["71423\n","71423\n","71423\n"]}],"source":["print(len(train_texts))\n","print(len(train_queries))\n","print(len(train_answers))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xPrlBV3hkJSe","outputId":"b80e02b2-7db4-44bc-b730-3aaf417cd7ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Passage:  ক্যাথলিকরা দলটির চারপাশে জড়ো হয়ে রেডিওতে খেলা শুনত, বিশেষ করে যখন এটি আমেরিকার প্রোটেস্ট্যান্ট প্রতিষ্ঠানের প্রতীক যে স্কুলগুলিকে আঘাত করেছিল  হার্ভার্ড, ইয়েল, প্রিন্সটন এবং আর্মি।\n","Query:  ক্যাথলিকরা নটরডামের সাথে যুক্ত, কোন ধর্মীয় গোষ্ঠীকে মানুষ মনে করে যে ইয়েল প্রতিনিধিত্ব করে?\n","Answer:  প্রোটেস্ট্যান্ট প্রতিষ্ঠানের\n"]}],"source":["x=100\n","print(\"Passage: \",train_texts[x])  \n","print(\"Query: \",train_queries[x])\n","print(\"Answer: \",train_answers[x])"]},{"cell_type":"markdown","metadata":{"id":"6JCAvOgalXbh"},"source":["As you can see we have 20302 passages, queries and answers from the validation data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bMs6opQPlLBE","outputId":"8853ac3c-f3b2-48d0-deb6-9c6ce8eda568"},"outputs":[{"name":"stdout","output_type":"stream","text":["7344\n","7344\n","7344\n"]}],"source":["print(len(val_texts))\n","print(len(val_queries))\n","print(len(val_answers))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FBf0eNdblL_P","outputId":"bcdd3a5b-9d33-42de-ff27-ce60f3168255"},"outputs":[{"name":"stdout","output_type":"stream","text":["Passage:  এটি উত্তর-পশ্চিমে আর্মেনিয়া, প্রকৃতপক্ষে নাগোরনো-কারাবাখ প্রজাতন্ত্র এবং আজারবাইজান; উত্তরে কাশ্মীর এবং রাশিয়া কাস্পিয়ান সাগর জুড়ে; উত্তর-পূর্বে তুর্কমেনিস্তান; পূর্ব দিকে আফগানিস্তান এবং পাকিস্তান; দক্ষিণে পারস্য উপসাগর এবং ওমান উপসাগর; এবং পশ্চিমে তুরস্ক এবং ইরাক।\n","Query:  ইরানের উত্তর-পশ্চিমে ইরানের সাথে কোন দেশ সীমান্তবর্তী?\n","Answer:  আর্মেনিয়া\n"]}],"source":["print(\"Passage: \",val_texts[0])  \n","print(\"Query: \",val_queries[0])\n","print(\"Answer: \",val_answers[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QsxbrdQBIN4l"},"outputs":[],"source":["# train_texts = train_texts[:10000]\n","# train_queries = train_queries[:10000]\n","# train_answers = train_answers[:10000]\n","\n","# val_texts = val_texts[:1000]\n","# val_queries = val_queries[:1000]\n","# val_answers = val_answers[:1000]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lq4GEXQ7IpwS"},"outputs":[],"source":["# print(len(train_texts))\n","# print(len(train_queries))\n","# print(len(train_answers))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MQ3KHKIkIy0G"},"outputs":[],"source":["# print(len(val_texts))\n","# print(len(val_queries))\n","# print(len(val_answers))"]},{"cell_type":"markdown","source":["## ***Step 3:*** Format according to input of train data"],"metadata":{"id":"rejked1pJ2kX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlexBsEcnI77"},"outputs":[],"source":["for i in range(len(train_texts)):\n","    real_answer = train_answers[i]\n","    train_texts[i] = train_texts[i] + \" <sep> \" + real_answer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xe2_AZszsRqr"},"outputs":[],"source":["for i in range(len(val_texts)):\n","    real_answer = val_answers[i]\n","    val_texts[i] = val_texts[i] + \" <sep> \" + real_answer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H20ASxHplRng"},"outputs":[],"source":["train_dataset = {'source_text': train_texts, 'target_text': train_queries}\n","val_dataset = {'source_text': val_texts, 'target_text': val_queries}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xq-TDc1pq9Aa"},"outputs":[],"source":["from datasets import Dataset\n","train_dataset = Dataset.from_dict(train_dataset)\n","val_dataset = Dataset.from_dict(val_dataset)"]},{"cell_type":"markdown","metadata":{"id":"83nKp3c5tU7A"},"source":["## ***Step 4:*** Tokenize passages and queries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_zvwBSPlQkt"},"outputs":[],"source":["class DataProcessor:\n","    def __init__(self, tokenizer, max_source_length=512, max_target_length=32):\n","        self.tokenizer = tokenizer\n","        self.max_source_length = max_source_length\n","        self.max_target_length = max_target_length\n","        self.sep_token = \"<sep>\"\n","  \n","    def process(self, dataset):\n","\n","        dataset = dataset.map(self._add_eos_examples)\n","        dataset = dataset.map(self._convert_to_features, batched=True)\n","        \n","        return dataset\n","  \n","    def _add_eos_examples(self, example):\n","        example['source_text'] = example['source_text'] + \" </s>\"\n","        example['target_text'] = example['target_text'] + \" </s>\"\n","        return example\n","  \n","    # tokenize the examples\n","    def _convert_to_features(self, example_batch):\n","        source_encoding = self.tokenizer.batch_encode_plus(\n","            example_batch['source_text'],\n","            max_length=self.max_source_length,\n","            padding='max_length',\n","            pad_to_max_length=True,\n","            truncation=True, \n","        )\n","        target_encoding = self.tokenizer.batch_encode_plus(\n","            example_batch['target_text'],\n","            max_length=self.max_target_length,\n","            padding='max_length',\n","            pad_to_max_length=True,\n","            truncation=True, \n","        )\n","\n","        encodings = {\n","            'source_ids': source_encoding['input_ids'], \n","            'target_ids': target_encoding['input_ids'],\n","            'attention_mask': source_encoding['attention_mask'],\n","        }\n","\n","        return encodings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dncUoMhtpWj"},"outputs":[],"source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","tokenizer = T5Tokenizer.from_pretrained(\"csebuetnlp/banglat5\")\n","tokenizer.add_tokens(['<sep>'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tG7soo7plLhH"},"outputs":[],"source":["processor = DataProcessor(\n","        tokenizer,\n","        max_source_length=512,\n","        max_target_length=64\n","    )\n","\n","train_dataset_final = processor.process(train_dataset)\n","val_dataset_final = processor.process(val_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gLUP5dauQO2"},"outputs":[],"source":["columns = [\"source_ids\", \"target_ids\", \"attention_mask\"]\n","train_dataset_final.set_format(type='torch', columns=columns)\n","val_dataset_final.set_format(type='torch', columns=columns)"]},{"cell_type":"markdown","source":["## ***Step 5:*** Training"],"metadata":{"id":"g6gQGxb3KtoQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZVEE-TyAy0f"},"outputs":[],"source":["question_generator_trainer_args=dict(\n","    debug=False,\n","    model_name=\"csebuetnlp/banglat5\",\n","    model_path=\"csebuetnlp/banglat5\",\n","    output_dir=\"./QGmodel\",\n","    data_dir=\"./QGdata\",\n","    train_file=None,\n","    config_name=\"csebuetnlp/banglat5\",\n","    tokenizer_name=\"csebuetnlp/banglat5\",\n","    cache_dir=\"./QGdata\",\n","    max_seq_length=512,\n","    doc_stride=128,\n","    max_query_length=64,\n","    max_ans_length=30,\n","    do_lower_case=False,\n","    train_batch_size=16,\n","    dev_batch_size=16,\n","    dynamic_batching=False,\n","    learning_rate=2e-4,\n","    gradient_accumulation_steps=1,\n","    weight_decay=0.0,\n","    max_grad_norm=1.0,\n","    num_train_epochs=3,\n","    max_steps=-1,\n","    warmup_steps=0,\n","    verbose_logging=False,\n","    train_logging_steps=2000,\n","    dev_logging_steps=2000,\n","    evaluate_during_training=True,\n","    save_steps=2000,\n","    no_cuda=False,\n","    overwrite_output_dir=True,\n","    overwrite_cache=False,\n","    seed=42,\n","    threads=1,\n",")\n","\n","from types import SimpleNamespace\n","\n","qg_trainer_args = SimpleNamespace(**question_generator_trainer_args)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PIvlscCeBKfP"},"outputs":[],"source":["import logging as logger\n","import os\n","from tqdm.notebook import tqdm, trange\n","from functools import partial\n","\n","import torch\n","from torch.utils.data import TensorDataset, SequentialSampler, DataLoader, RandomSampler\n","\n","from torch.optim import AdamW\n","from transformers import (\n","    T5Config,\n","    T5Tokenizer,\n","    T5ForConditionalGeneration,\n","    get_linear_schedule_with_warmup,\n",")\n","\n","from torch.utils.tensorboard import SummaryWriter\n","\n","\n","logger.basicConfig(level=logger.INFO)\n","\n","\n","\n","def train(args, train_dataset, dev_dataset, model, tokenizer):\n","    \"\"\" Train the model \"\"\"\n","    args.device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n","    \n","    model.to(args.device)\n","\n","    tb_writer = SummaryWriter(os.path.join(args.output_dir, 'TB_writer'))\n","\n","    Rsampler = RandomSampler(train_dataset)\n","    train_dataloader = DataLoader(train_dataset, sampler=Rsampler, batch_size=args.train_batch_size)\n","\n","    Ssampler = SequentialSampler(dev_dataset)\n","    dev_dataloader = DataLoader(dev_dataset, sampler=Ssampler, batch_size=args.dev_batch_size)\n","\n","    if args.max_steps > 0:\n","        t_total = args.max_steps\n","        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n","    else:\n","        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n","    )\n","\n","    # Check if saved optimizer or scheduler states exist\n","    if os.path.isfile(os.path.join(args.model_path, \"optimizer.pt\")) and os.path.isfile(\n","        os.path.join(args.model_path, \"scheduler.pt\")\n","    ):\n","        # Load in optimizer and scheduler states\n","        optimizer.load_state_dict(torch.load(os.path.join(args.model_path, \"optimizer.pt\")))\n","        scheduler.load_state_dict(torch.load(os.path.join(args.model_path, \"scheduler.pt\")))\n","\n","    # Train!\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_dataset))\n","    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n","    logger.info(\"  Batch size = %d\", args.train_batch_size)\n","    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n","    logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 1\n","    epochs_trained = 0\n","    steps_trained_in_current_epoch = 0\n","\n","    # Check if continuing training from a checkpoint\n","    if os.path.exists(args.model_path):\n","        try:\n","            # set global_step to gobal_step of last saved checkpoint from model path\n","            checkpoint_suffix = args.model_path.split(\"-\")[-1].split(\"/\")[0]\n","            global_step = int(checkpoint_suffix)\n","            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n","            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n","\n","            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n","            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n","            logger.info(\"  Continuing training from global step %d\", global_step)\n","            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n","        except ValueError:\n","            logger.info(\"  Starting fine-tuning.\")\n","\n","    model.train()\n","    model.zero_grad()\n","    train_iterator = trange(epochs_trained, int(args.num_train_epochs), desc=\"Epoch\")\n","\n","    # Added here for reproductibility\n","    # set_seed(args)\n","\n","    loss_cum = None\n","    # torch.autograd.set_detect_anomaly(True)\n","    for _ in train_iterator:\n","\n","        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", smoothing=0.05)\n","        for step, batch in enumerate(epoch_iterator):\n","\n","            # Skip past any already trained steps if resuming training\n","            if steps_trained_in_current_epoch > 0:\n","                steps_trained_in_current_epoch -= 1\n","                continue\n","\n","\n","            inputs = {\n","                \"input_ids\": (batch['source_ids']).to(args.device),\n","                \"attention_mask\": (batch['attention_mask']).to(args.device),\n","                \"labels\": (batch['target_ids']).to(args.device)\n","            }\n","\n","            outputs = model(**inputs)\n","            loss = outputs.loss\n","\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            \n","            loss.backward()\n","            if loss_cum is None:\n","                loss_cum = loss.detach()\n","            else:\n","                loss_cum += loss.detach()\n","\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                \n","                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","\n","                # Log train metrics\n","                if (not global_step % args.train_logging_steps) and args.train_logging_steps > 0:\n","                    logger.info(\"train_loss %lf global step %d\", loss_cum.item() / args.train_logging_steps, global_step)\n","                    tb_writer.add_scalar('train_loss', loss_cum.item() / args.train_logging_steps, global_step)\n","\n","                    loss_cum = None\n","                # Log dev metrics\n","                if args.dev_logging_steps > 0 and global_step % args.dev_logging_steps == 0 and args.evaluate_during_training:\n","                    dev_loss = evaluate(args, dev_dataset, model)\n","                    logger.info(\"dev_loss %lf %d\", dev_loss, global_step)\n","                    tb_writer.add_scalar(\"dev_loss\", dev_loss, global_step)\n","                    tb_writer.add_scalar(\"lr\", scheduler.get_last_lr()[0], global_step)\n","\n","                # Save model checkpoint\n","                if args.save_steps > 0 and global_step % args.save_steps == 0:\n","                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n","                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n","\n","                    model.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(output_dir)\n","\n","                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n","                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n","                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n","\n","            if args.max_steps > 0 and global_step > args.max_steps:\n","                epoch_iterator.close()\n","                break\n","        if args.max_steps > 0 and global_step > args.max_steps:\n","            train_iterator.close()\n","            break\n","\n","        tb_writer.close()\n","\n","\n","def evaluate(args, dev_dataset, model):\n","    \"\"\" Evaluate loss on the dev set \"\"\"\n","\n","    Ssampler = SequentialSampler(dev_dataset)\n","    dev_dataloader = DataLoader(dev_dataset, sampler=Ssampler, batch_size=8)\n","\n","    model.eval()\n","    iterator = tqdm(dev_dataloader, desc=\"Evaluation\", smoothing=0.05)\n","    loss_cum = None\n","    num_batch = 0\n","    for step, batch in enumerate(iterator):\n","        num_batch += 1\n","\n","        inputs = {\n","            \"input_ids\": (batch['source_ids']).to(args.device),\n","            \"attention_mask\": (batch['attention_mask']).to(args.device),\n","            \"labels\": (batch['target_ids']).to(args.device)\n","        }\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            if loss_cum is None:\n","                loss_cum = outputs.loss\n","            else:\n","                loss_cum += outputs.loss\n","\n","    model.train()\n","\n","    return loss_cum.item() / num_batch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["734cb59f989c4dd9860ca8e69a092698","64a2ae2232df48ef9911f82f1085bf2d","27cfdb721a8845c9a092f4362df96fa6","e6afef6afa5948cba620f37453de9b68","28afbae050484d90a8990fb95f2fb13b","17b491a8b43d4a1bb1eed442ec41db5d","dd4bd6bc6aee4ba4b420cc1441340996","2b6420eedbd24fdbb4e71dd7affe477c","9165c8da1e80486581b182329d6351ac","2f3708101aed4d03ba5caf7ef81c2d00","7d48754a6b0945b7b46951e33e379a10"]},"id":"mgTxvdUQ4Nng","outputId":"09359905-2837-44de-f74a-ae7626276258"},"outputs":[{"data":{"text/plain":["Embedding(32101, 768)"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import T5Config, T5ForConditionalGeneration\n","# Load pretrained model and tokenizer\n","config = T5Config.from_pretrained(\n","    qg_trainer_args.config_name if qg_trainer_args.config_name else qg_trainer_args.model_path,\n","    # cache_dir=qg_trainer_args.cache_dir if qg_trainer_args.cache_dir else None,\n",")\n","tokenizer = T5Tokenizer.from_pretrained(\n","    qg_trainer_args.tokenizer_name if qg_trainer_args.tokenizer_name else qg_trainer_args.model_path,\n","    do_lower_case=qg_trainer_args.do_lower_case\n","    # cache_dir=qg_trainer_args.cache_dir if qg_trainer_args.cache_dir else None,\n",")\n","tokenizer.add_tokens(['<sep>'])\n","model = T5ForConditionalGeneration.from_pretrained(\n","    qg_trainer_args.model_path,\n","    from_tf=bool(\".ckpt\" in qg_trainer_args.model_path),\n","    config=config\n","    # cache_dir=qg_trainer_args.cache_dir if qg_trainer_args.cache_dir else None,\n",")\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427,"referenced_widgets":["13f6f1e21aee40648e61008d003a68b8","7cd7e7356e854e30bbdf7277718ef3b8","cc62528be53d4335907b44f966a9707f","eb7659d0457c4d0cbf36339dd0db6000","8cf9a578526a46f3879cd607cc856d83","45c97089ad7e4ca4bbe87fae05d1de79","8412e46a69c249cfae068249bdd0a3a0","6054baffd555472f992aaf8113494b6f","790d182ddd71461080bccb6e009c2562","8c09866c73804e6abd624e85151b0a55","4586d96eacc943a682fc6ff9ee21f96e","98c5425b4c934096ba4ed2d4b4f291e5","e5ec4f0eb1fb4502a86f21a861cdf957","28697bc5f5434b3db6bd32a8e8390fc2","2845cd0af9f941c79e8c7316669d515a","6ecea1df20c2404da8d98456e37d8a2c","60b1500bed4e4a9696afb8316c650a1a","f994a1d40c5c40fa8ba457f6807fb92b","8f0a9f214e7a4b14aa705099869071c2","e11efed7bcc0453fac271c73e79fa2b2","9e4191b8034849beb51555d9953b49f5","a1bc7a1d5c0d423f9000240dcb958bdd","49071aaf4209404489373a0fd2916cd9","e8f251949bc94a4fb7a1456ebab9450d","daaab4cb4a0549dbabb58d44050e5cc5","336d20964ef94fb58cb7664e5d3d3f64","4d7578004ad243ad8be51736a9b4db9d","946b575740b54184b74e1a12b052340d","188a397bb36549c19f195be0644e33e9","12daa4c8a5314ef8b97b5766217ecfd9","6d3df1857cb042b98403c23158be9138","d9db3a0a9ecc4a7f8b667c46610ee0ac"]},"id":"v0riPG8VCkt5","outputId":"8349de66-3a07-441c-d332-0672ba1ba67b"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:root:***** Running training *****\n","INFO:root:  Num examples = 71423\n","INFO:root:  Num Epochs = 3\n","INFO:root:  Batch size = 16\n","INFO:root:  Gradient Accumulation steps = 1\n","INFO:root:  Total optimization steps = 13392\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49071aaf4209404489373a0fd2916cd9","version_major":2,"version_minor":0},"text/plain":["Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8f251949bc94a4fb7a1456ebab9450d","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/4464 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:root:train_loss 3.036305 global step 2000\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"daaab4cb4a0549dbabb58d44050e5cc5","version_major":2,"version_minor":0},"text/plain":["Evaluation:   0%|          | 0/918 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:root:dev_loss 0.460082 2000\n","INFO:root:Saving model checkpoint to ./QGmodel/checkpoint-2000\n","INFO:root:train_loss 0.526687 global step 4000\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"336d20964ef94fb58cb7664e5d3d3f64","version_major":2,"version_minor":0},"text/plain":["Evaluation:   0%|          | 0/918 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:root:dev_loss 0.432929 4000\n","INFO:root:Saving model checkpoint to ./QGmodel/checkpoint-4000\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d7578004ad243ad8be51736a9b4db9d","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/4464 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:root:train_loss 0.482775 global step 6000\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"946b575740b54184b74e1a12b052340d","version_major":2,"version_minor":0},"text/plain":["Evaluation:   0%|          | 0/918 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:root:dev_loss 0.426978 6000\n","INFO:root:Saving model checkpoint to ./QGmodel/checkpoint-6000\n","INFO:root:train_loss 0.468506 global step 8000\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"188a397bb36549c19f195be0644e33e9","version_major":2,"version_minor":0},"text/plain":["Evaluation:   0%|          | 0/918 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:root:dev_loss 0.421188 8000\n","INFO:root:Saving model checkpoint to ./QGmodel/checkpoint-8000\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12daa4c8a5314ef8b97b5766217ecfd9","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/4464 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:root:train_loss 0.449008 global step 10000\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d3df1857cb042b98403c23158be9138","version_major":2,"version_minor":0},"text/plain":["Evaluation:   0%|          | 0/918 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:root:dev_loss 0.418560 10000\n","INFO:root:Saving model checkpoint to ./QGmodel/checkpoint-10000\n","INFO:root:train_loss 0.437592 global step 12000\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9db3a0a9ecc4a7f8b667c46610ee0ac","version_major":2,"version_minor":0},"text/plain":["Evaluation:   0%|          | 0/918 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:root:dev_loss 0.416925 12000\n","INFO:root:Saving model checkpoint to ./QGmodel/checkpoint-12000\n"]}],"source":["train(qg_trainer_args, train_dataset_final, val_dataset_final, model, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"Lw2keLJj8ncA","outputId":"fb08734f-52eb-4a86-e673-1c8c1027383e"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'১৯৮৭ সালে, যখন কিছু ছাত্র বিশ্বাস করেছিলেন যে দ্য ওভারভারভার একটি রক্ষণশীল পজিশন প্রদর্শন করতে শুরু করেছিল, একটি লিবারাল পত্রিকা, কমন সান্সেস প্রকাশিত হয়েছিল। <sep> ১৯৮৭'"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["train_texts[9]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L8Jh3cAW4ef5","outputId":"e3304349-50d8-4668-ad32-723f1951bcf5"},"outputs":[{"name":"stdout","output_type":"stream","text":["টেলিভিশন স্টেশন, এনডিটিভি, ২০০২ সালে একটি শো থেকে ২০০৬ সালের সেপ্টেম্বরের মধ্যে মূল প্রোগ্রামিং সহ একটি পূর্ণ ২৪ ঘন্টা চ্যানেলে পরিণত হয়েছিল। <sep> এনডিটিভি\n","কোন টেলিভিশন স্টেশন ২০০৬ সালের সেপ্টেম্বরে একটি সম্পূর্ণ ২৪ ঘন্টা প্রোগ্রামিং সহ একটি মূল চ্যানেলে পরিণত হয়েছিল?\n","কোন টেলিভিশন স্টেশন ২০০৬ সালের সেপ্টেম্বরে পূর্ণ ২৪ ঘন্টা সম্প্রচার শুরু করে?\n","কোন টেলিভিশন স্টেশন ২০০৬ সালের সেপ্টেম্বরে পূর্ণ ২৪ ঘন্টা সম্প্রচার করে?\n","কোন টেলিভিশন স্টেশন ২০০৬ সালের সেপ্টেম্বরে একটি সম্পূর্ণ ২৪ ঘন্টা প্রোগ্রামিং সহ একটি মূল চ্যানেল হয়ে ওঠে?\n","কোন টেলিভিশন স্টেশন ২০০৬ সালের সেপ্টেম্বরে পূর্ণ ২৪ ঘন্টা সম্প্রচার করেছিল?\n"]}],"source":["device = torch.device(\"cuda\")\n","model.to(device)\n","text = train_texts[245]\n","print(text)\n","input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n","outputs = model.generate(input_ids.to(device),\n","                        max_length=50, \n","                        num_beams=5, \n","                        no_repeat_ngram_size=2, \n","                        num_return_sequences=5, \n","                        early_stopping=True)\n","for output in outputs:\n","    print(tokenizer.decode(output, skip_special_tokens=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EZeG0xyAHP2r","outputId":"9c8cd35d-0cb8-423f-c61c-b4afdbca717e"},"outputs":[{"data":{"text/plain":["'./QGmodel'"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["qg_trainer_args.output_dir"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7k4hP3pzP0xP"},"outputs":[],"source":["model.save_pretrained(qg_trainer_args.output_dir)\n","tokenizer.save_pretrained(qg_trainer_args.output_dir)\n","torch.save(qg_trainer_args, os.path.join(qg_trainer_args.output_dir, \"training_args.bin\"))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"13f6f1e21aee40648e61008d003a68b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7cd7e7356e854e30bbdf7277718ef3b8","IPY_MODEL_cc62528be53d4335907b44f966a9707f","IPY_MODEL_eb7659d0457c4d0cbf36339dd0db6000"],"layout":"IPY_MODEL_8cf9a578526a46f3879cd607cc856d83"}},"17b491a8b43d4a1bb1eed442ec41db5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27cfdb721a8845c9a092f4362df96fa6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b6420eedbd24fdbb4e71dd7affe477c","max":1200794589,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9165c8da1e80486581b182329d6351ac","value":1200794589}},"2845cd0af9f941c79e8c7316669d515a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e4191b8034849beb51555d9953b49f5","placeholder":"​","style":"IPY_MODEL_a1bc7a1d5c0d423f9000240dcb958bdd","value":" 21/8775 [00:14&lt;1:31:56,  1.59it/s]"}},"28697bc5f5434b3db6bd32a8e8390fc2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f0a9f214e7a4b14aa705099869071c2","max":8775,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e11efed7bcc0453fac271c73e79fa2b2","value":21}},"28afbae050484d90a8990fb95f2fb13b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b6420eedbd24fdbb4e71dd7affe477c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f3708101aed4d03ba5caf7ef81c2d00":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4586d96eacc943a682fc6ff9ee21f96e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45c97089ad7e4ca4bbe87fae05d1de79":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6054baffd555472f992aaf8113494b6f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60b1500bed4e4a9696afb8316c650a1a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64a2ae2232df48ef9911f82f1085bf2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17b491a8b43d4a1bb1eed442ec41db5d","placeholder":"​","style":"IPY_MODEL_dd4bd6bc6aee4ba4b420cc1441340996","value":"Downloading: 100%"}},"6ecea1df20c2404da8d98456e37d8a2c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"734cb59f989c4dd9860ca8e69a092698":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_64a2ae2232df48ef9911f82f1085bf2d","IPY_MODEL_27cfdb721a8845c9a092f4362df96fa6","IPY_MODEL_e6afef6afa5948cba620f37453de9b68"],"layout":"IPY_MODEL_28afbae050484d90a8990fb95f2fb13b"}},"790d182ddd71461080bccb6e009c2562":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7cd7e7356e854e30bbdf7277718ef3b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45c97089ad7e4ca4bbe87fae05d1de79","placeholder":"​","style":"IPY_MODEL_8412e46a69c249cfae068249bdd0a3a0","value":"Epoch:   0%"}},"7d48754a6b0945b7b46951e33e379a10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8412e46a69c249cfae068249bdd0a3a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c09866c73804e6abd624e85151b0a55":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cf9a578526a46f3879cd607cc856d83":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f0a9f214e7a4b14aa705099869071c2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9165c8da1e80486581b182329d6351ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98c5425b4c934096ba4ed2d4b4f291e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e5ec4f0eb1fb4502a86f21a861cdf957","IPY_MODEL_28697bc5f5434b3db6bd32a8e8390fc2","IPY_MODEL_2845cd0af9f941c79e8c7316669d515a"],"layout":"IPY_MODEL_6ecea1df20c2404da8d98456e37d8a2c"}},"9e4191b8034849beb51555d9953b49f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1bc7a1d5c0d423f9000240dcb958bdd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc62528be53d4335907b44f966a9707f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_6054baffd555472f992aaf8113494b6f","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_790d182ddd71461080bccb6e009c2562","value":0}},"dd4bd6bc6aee4ba4b420cc1441340996":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e11efed7bcc0453fac271c73e79fa2b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e5ec4f0eb1fb4502a86f21a861cdf957":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60b1500bed4e4a9696afb8316c650a1a","placeholder":"​","style":"IPY_MODEL_f994a1d40c5c40fa8ba457f6807fb92b","value":"Iteration:   0%"}},"e6afef6afa5948cba620f37453de9b68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f3708101aed4d03ba5caf7ef81c2d00","placeholder":"​","style":"IPY_MODEL_7d48754a6b0945b7b46951e33e379a10","value":" 1.20G/1.20G [00:20&lt;00:00, 61.3MB/s]"}},"eb7659d0457c4d0cbf36339dd0db6000":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c09866c73804e6abd624e85151b0a55","placeholder":"​","style":"IPY_MODEL_4586d96eacc943a682fc6ff9ee21f96e","value":" 0/2 [00:14&lt;?, ?it/s]"}},"f994a1d40c5c40fa8ba457f6807fb92b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"markdown","source":["<h1> This notebook translates SQuAD 2.0 to Bangla then corrects and aligns answer spans. This notebook needs translation of SQuaD 1.1 done before to copy already translated contexts</h1>\n","\n","*   In the translation section select train.json or dev.json to translate train or dev split.\n","\n","*   Select low and high values to translate a part of dataset if low=0 and high=40 it means the first 40 paragraphs will be translated. The translated dictionary will be saved as \"squad2_translated_\"+str(low)+\"_\"+str(high)+\"_dict.json\"\n","\n","*   In the merge translation section marge the translated dataset parts giving the correct filename and running the subsection merge parts as many times as the number of parts generated in the previous section. The full dataset is saved as \"squad2_translated_temp.json\"\n","\n","*   Finally run the answer correction and alignment section.\n","\n","\n"],"metadata":{"id":"uD_yhYlszmmk"},"id":"uD_yhYlszmmk"},{"cell_type":"code","source":["!pip install transformers==\"4.25.1\" sentencepiece==\"0.1.97\" fasttext==\"0.9.2\" utoken==\"0.1.8\" nltk==\"3.8.1\" torch==\"1.13.1+cu116\" numpy==\"1.21.6\" tqdm==\"4.64.1\" --quiet"],"metadata":{"id":"z3PRKyZ2zqLy"},"id":"z3PRKyZ2zqLy","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"a40ac439","metadata":{"id":"a40ac439"},"outputs":[],"source":["# import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""]},{"cell_type":"code","execution_count":null,"id":"c48dd4fe","metadata":{"id":"c48dd4fe"},"outputs":[],"source":["import numpy as np\n","from tqdm.notebook import tqdm, trange\n","import json\n","import nltk"]},{"cell_type":"code","execution_count":null,"id":"b49264af","metadata":{"id":"b49264af"},"outputs":[],"source":["!mkdir squad2_data\n","!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O ./squad2_data/train.json\n","!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O ./squad2_data/dev.json"]},{"cell_type":"code","execution_count":null,"id":"8dbe7b5f","metadata":{"id":"8dbe7b5f"},"outputs":[],"source":["f2 = open(\"./squad2_data/train.json\")\n","data2 = json.load(f2)\n","f2.close()"]},{"cell_type":"code","execution_count":null,"id":"a5ce4eb4","metadata":{"id":"a5ce4eb4"},"outputs":[],"source":["f1 = open('./squad1_data/squad1_translated_merged_temp.json','r')\n","data1 = json.load(f1)\n","f1.close()"]},{"cell_type":"code","source":["save_loc = './squad2_data'"],"metadata":{"id":"gzX-zq0LGxfN"},"id":"gzX-zq0LGxfN","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"36d06e5d","metadata":{"id":"36d06e5d"},"outputs":[],"source":["data2['data'][0]['paragraphs'][0]['context']"]},{"cell_type":"markdown","source":["# **Translation**"],"metadata":{"id":"oSnR3gfaG0A9"},"id":"oSnR3gfaG0A9"},{"cell_type":"code","execution_count":null,"id":"8acc305f","metadata":{"id":"8acc305f","outputId":"e0a96c81-4b9e-4a62-b54a-f2bfea45ceeb"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-12-30 02:52:55.537591: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-12-30 02:52:55.653964: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2022-12-30 02:52:55.671285: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2022-12-30 02:52:55.991475: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64\n","2022-12-30 02:52:55.991510: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64\n","2022-12-30 02:52:55.991513: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"]}],"source":["from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline"]},{"cell_type":"code","execution_count":null,"id":"5d5058d9","metadata":{"id":"5d5058d9"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\n","    \"pretrained_models/nllb-200-3.3B\", src_lang=\"eng_Latn\", tgt_lang=\"ben_Beng\"\n",")"]},{"cell_type":"code","execution_count":null,"id":"faa51ef1","metadata":{"id":"faa51ef1"},"outputs":[],"source":["model = AutoModelForSeq2SeqLM.from_pretrained(\"pretrained_models/nllb-200-3.3B\")"]},{"cell_type":"code","execution_count":null,"id":"63d7c84b","metadata":{"id":"63d7c84b"},"outputs":[],"source":["translator = pipeline('translation', model=model, tokenizer=tokenizer, src_lang=\"eng_Latn\", tgt_lang=\"ben_Beng\", device=\"cuda:0\")"]},{"cell_type":"code","execution_count":null,"id":"ffb84a23","metadata":{"id":"ffb84a23"},"outputs":[],"source":["# nltk.download('punkt')\n","def get_c_sent_list(c_src):\n","  c_src_sent_list = nltk.sent_tokenize(c_src)\n","  # print(a_src)\n","  c_tran_sent_list = []\n","\n","  for sent in c_src_sent_list:\n","    c_tran_sent = translator(sent, max_length=400)[0]['translation_text']\n","    c_tran_sent_list.append(c_tran_sent)\n","\n","  return c_tran_sent_list, c_src_sent_list"]},{"cell_type":"code","execution_count":null,"id":"0efcbb36","metadata":{"id":"0efcbb36","outputId":"36c31af8-61a2-4698-db76-24b77712d83f"},"outputs":[{"name":"stdout","output_type":"stream","text":["['নরম্যান (Norman: Nourmands; French: Normands; Latin: Normanni) ছিল একদল মানুষ যারা দশম ও একাদশ শতকে ফ্রান্সের একটি অঞ্চল নরম্যান্ডিতে তাদের নাম দিয়েছিল।', 'তারা ডেনমার্ক, আইসল্যান্ড এবং নরওয়ের নর্স (\"নর্মান\" \"নর্সেম্যান\" থেকে এসেছে) আক্রমণকারী এবং জলদস্যুদের বংশধর ছিলেন যারা তাদের নেতা রোলোর অধীনে পশ্চিম ফ্রাঙ্কার রাজা তৃতীয় চার্লসের প্রতি আনুগত্যের শপথ নিতে সম্মত হন।', 'বহু প্রজন্ম ধরে স্থানীয় ফ্রাঙ্ক এবং রোমান-গল জনগোষ্ঠীর সাথে মিশ্রিত হওয়ার ফলে তাদের বংশধররা ক্রমশ পশ্চিম ফ্রাঙ্কার ক্যারোলিংজিয়ান-ভিত্তিক সংস্কৃতির সাথে মিশ্রিত হবে।', 'নরম্যানদের স্বতন্ত্র সাংস্কৃতিক ও জাতিগত পরিচয় প্রথম দিকে দশম শতাব্দীর প্রথমার্ধে আবির্ভূত হয়েছিল এবং পরবর্তী শতাব্দীতে এটি বিকশিত হতে থাকে।']\n"]}],"source":["# Test Run\n","output,_ = get_c_sent_list(data2['data'][0]['paragraphs'][0]['context'])\n","print(output)"]},{"cell_type":"code","execution_count":null,"id":"e556b0a8","metadata":{"id":"e556b0a8","outputId":"acf47e52-6e5d-4086-9db3-38403d298228"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/dlpc01/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","import re\n","def word_tokenize(text):\n","    tokens = nltk.word_tokenize(text)\n","    new_tokens = []\n","    for token in tokens:\n","        if token[-1] == \"।\" and len(token)>1:\n","            token_1 = token[:-1]\n","            token_2 = token[-1]\n","            new_tokens.append(token_1)\n","            new_tokens.append(token_2)\n","        elif (\"-\" in token or  \"–\" in token) and len(token)>1:\n","            temp_tokens = token.replace(\"-\", \" - \").replace(\"–\", \" – \").replace(\"—\",\" — \").split(\" \")\n","            new_tokens.extend(temp_tokens)\n","        else:\n","            new_tokens.append(token)\n","    return new_tokens\n","\n","def detokenize(words):\n","    \"\"\"\n","    Untokenizing a text undoes the tokenizing operation, restoring\n","    punctuation and spaces to the places that people expect them to be.\n","    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n","    except for line breaks.\n","    \"\"\"\n","    text = ' '.join(words)\n","    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('‘ ', '‘').replace(' ’', '’').replace('. . .',  '...')\n","    step2 = step1.replace(\"( \", \"(\").replace(\" )\", \")\").replace(\" - \",\"-\").replace(\" – \",\"–\").replace(\" — \",\"—\")\n","    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n","    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n","    step5 = step4.replace(\" ' \", \"'\").replace(\" & \", \"&\")\n","    step6 = step5.replace(\" ` \", \" '\").replace(\"“ \", \"“\").replace(\" ”\", \"”\")\n","    step7 = step6.replace(\" ।\", \"।\")\n","    step8 = step7.replace(\" %\",\"%\").replace(\"[ \", \"[\").replace(\" ]\", \"]\").replace(\"{ \", \"{\").replace(\" }\", \"}\")\n","    return step8.strip()\n"]},{"cell_type":"code","execution_count":null,"id":"58738b9e","metadata":{"id":"58738b9e"},"outputs":[],"source":["# Translating any left-over english digits\n","digits_en = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n","digits_bn = ['০', '১', '২', '৩', '৪', '৫', '৬', '৭', '৮', '৯']\n","def translate_digits(s):\n","  s = list(s)\n","  for i in range(len(s)):\n","    if s[i] in digits_en:\n","      s[i] = digits_bn[digits_en.index(s[i])]\n","  s = \"\".join(s)\n","  return s"]},{"cell_type":"code","execution_count":null,"id":"eba61491","metadata":{"id":"eba61491"},"outputs":[],"source":["def get_index_c_tran_with_ans(c_src_tokens, c_tran_tokens, a_ch_start_pos_src):\n","  ch_len = 0\n","  for ind in range(len(c_src_tokens)):\n","    ch_len += len(c_src_tokens[ind])\n","    ch_len += 1 # For space after a sentence\n","    if(ch_len > a_ch_start_pos_src):\n","      return ind\n","  return ind"]},{"cell_type":"code","execution_count":null,"id":"bd282a41","metadata":{"id":"bd282a41"},"outputs":[],"source":["# a_src = data['data'][0]['paragraphs'][3]['qas'][1]['answers'][0]['text']\n","import string\n","import re\n","punc = '''!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~—।–'''\n","def get_a_tran(c_tran_with_ans, a_src):\n","  a_tran = a_src\n","  if a_src in c_tran_with_ans:\n","    a_tran = a_src\n","  elif len(word_tokenize(a_src))==1:\n","    clean_ans_text = a_src.translate(str.maketrans('', '', punc))\n","    if re.match('^[0-9]*$', clean_ans_text):\n","      a_tran = translate_digits(a_src)\n","  else:\n","    a_tran = translator(a_src, max_length=400)[0]['translation_text']\n","    a_tran = translate_digits(a_tran)\n","  return a_tran\n","\n","# print(c_tran_token)\n","# print(a_tran_token)"]},{"cell_type":"markdown","source":["# **Get Bangla Context from SQUAD 1.1 Translation or Translate**"],"metadata":{"id":"akz9SHM6E259"},"id":"akz9SHM6E259"},{"cell_type":"code","execution_count":null,"id":"1327eaaf","metadata":{"colab":{"referenced_widgets":["0b70a578e3b04686bbf1aa322168a76b"]},"id":"1327eaaf","outputId":"2e7ab756-105d-4638-8971-0d10c8597b70"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b70a578e3b04686bbf1aa322168a76b","version_major":2,"version_minor":0},"text/plain":["Paragraphs:   0%|          | 0/35 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["for di in tqdm(range(len(data2['data'])),desc=\"Paragraphs\"):\n","    d = data2['data'][di]\n","    for p in d['paragraphs']:\n","        flag = False\n","        for di1 in range(len(data1['data'])):\n","            for pi1 in range(len(data1['data'][di1]['paragraphs'])):\n","                if data1['data'][di1]['paragraphs'][pi1]['context'] == p['context']:\n","                    flag = True\n","                    break\n","            if flag:\n","                break\n","        if flag:\n","          p['bangla_context'] = data1['data'][di1]['paragraphs'][pi1]['bangla_context']\n","        else:\n","          c_tran_sent_list, c_src_sent_list = get_c_sent_list(p['context'])\n","          p['bangla_context'] = c_tran_sent_list"]},{"cell_type":"code","execution_count":null,"id":"cf2b2ecb","metadata":{"id":"cf2b2ecb"},"outputs":[],"source":["low = 0\n","high = 40\n","save_filename = \"squad2_translated_only_context_\"+str(low)+\"_\"+str(high)+\"_dict\"\n","save_loc = './squad2_data'"]},{"cell_type":"code","execution_count":null,"id":"61d25f14","metadata":{"id":"61d25f14"},"outputs":[],"source":["out_file = open(save_loc+'/'+save_filename+'.json', \"w\")\n","json.dump(data2, out_file, indent = 4) # save whole data replace parts later\n","out_file.close()"]},{"cell_type":"markdown","source":["# **Translate All Questions**"],"metadata":{"id":"CiDcIINMEybN"},"id":"CiDcIINMEybN"},{"cell_type":"code","execution_count":null,"id":"8b7c0c98","metadata":{"colab":{"referenced_widgets":["205ca37b1f5341a2929fc14352df484e"]},"id":"8b7c0c98","outputId":"c1d24da5-f8a6-41db-a849-5a6bab93cf8b"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"205ca37b1f5341a2929fc14352df484e","version_major":2,"version_minor":0},"text/plain":["Paragraphs:   0%|          | 0/35 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["q_src_all = []\n","for di in tqdm(range(0,len(data2['data'])),desc=\"Paragraphs\"):\n","  d = data2['data'][di]\n","  for p in d['paragraphs']:\n","    for qas in p['qas']:\n","        q_src_all.append(qas['question'])\n"]},{"cell_type":"code","execution_count":null,"id":"d0568342","metadata":{"id":"d0568342"},"outputs":[],"source":["# len(q_src_all)"]},{"cell_type":"code","execution_count":null,"id":"eced917a","metadata":{"id":"eced917a"},"outputs":[],"source":["import torch\n","class QDataset(torch.utils.data.Dataset):\n","    def __init__(self, text):\n","        self.text = text\n","\n","    def __getitem__(self, idx):\n","        return self.text[idx]\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    \n","q_dataset = QDataset(q_src_all)"]},{"cell_type":"code","execution_count":null,"id":"d72c280b","metadata":{"colab":{"referenced_widgets":["8e75fd39c6084806950c65122d92df66"]},"id":"d72c280b","outputId":"7e67b454-283b-4a2f-8604-ab1e0d38c883"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e75fd39c6084806950c65122d92df66","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/11873 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["q_tran_all = []\n","for out in tqdm(translator(q_dataset, batch_size=16), total=len(q_dataset)):\n","    q_tran_all.append(out)"]},{"cell_type":"code","execution_count":null,"id":"33e30317","metadata":{"id":"33e30317"},"outputs":[],"source":["q_tran_all = [q[0]['translation_text'] for q in q_tran_all]"]},{"cell_type":"code","execution_count":null,"id":"88841ee3","metadata":{"colab":{"referenced_widgets":["731571da698643af946e959d49c6f594"]},"id":"88841ee3","outputId":"775a9367-ed50-4189-bdee-1de6af5f2d61"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"731571da698643af946e959d49c6f594","version_major":2,"version_minor":0},"text/plain":["Paragraphs:   0%|          | 0/35 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["i=0\n","for di in tqdm(range(0,len(data2['data'])),desc=\"Paragraphs\"):\n","  d = data2['data'][di]\n","  for p in d['paragraphs']:\n","    for qas in p['qas']:\n","      qas['q_tran'] = q_tran_all[i]\n","      i+=1\n"]},{"cell_type":"code","execution_count":null,"id":"2bdbfba9","metadata":{"id":"2bdbfba9"},"outputs":[],"source":["# for di in tqdm(range(0,1),desc=\"Paragraphs\"):\n","#   d = data2['data'][di]\n","#   for p in tqdm(d['paragraphs'],desc=\"Contexts\"):\n","#     print(p)"]},{"cell_type":"code","execution_count":null,"id":"d0d69785","metadata":{"id":"d0d69785"},"outputs":[],"source":["# low = 0\n","# high = len(data2['data'])\n","# save_filename = \"SQuAD2_translated_cq_dev_\"+str(low)+\"_\"+str(high)+\"_dict\"\n","# save_loc = './squad2_data'"]},{"cell_type":"code","execution_count":null,"id":"feae4d42","metadata":{"id":"feae4d42"},"outputs":[],"source":["# out_file = open(save_loc+'/'+save_filename+'.json', \"w\")\n","# json.dump(data2, out_file, indent = 4) # save whole data replace parts later\n","# out_file.close()"]},{"cell_type":"markdown","source":["# **Translate All Answers**"],"metadata":{"id":"yqAqHHUAFICV"},"id":"yqAqHHUAFICV"},{"cell_type":"code","execution_count":null,"id":"3b0ef95f","metadata":{"colab":{"referenced_widgets":["743d2f43d2eb48038e0d650f8b1f2f58"]},"id":"3b0ef95f","outputId":"10c97cca-3075-4f48-f265-672fd3d6a706"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"743d2f43d2eb48038e0d650f8b1f2f58","version_major":2,"version_minor":0},"text/plain":["Paragraphs:   0%|          | 0/35 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["a_src_all = []\n","for di in tqdm(range(0,len(data2['data'])),desc=\"Paragraphs\"):\n","  d = data2['data'][di]\n","  for p in d['paragraphs']:\n","    for qas in p['qas']:\n","        for ans in qas['answers']:\n","          a_src_all.append(ans['text'])"]},{"cell_type":"code","execution_count":null,"id":"288593d2","metadata":{"id":"288593d2"},"outputs":[],"source":["import torch\n","class ADataset(torch.utils.data.Dataset):\n","    def __init__(self, text):\n","        self.text = text\n","\n","    def __getitem__(self, idx):\n","        return self.text[idx]\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    \n","a_dataset = ADataset(a_src_all)"]},{"cell_type":"code","execution_count":null,"id":"9c78a149","metadata":{"colab":{"referenced_widgets":["98babaf6e6c942cea9630bc1586c29ec"]},"id":"9c78a149","outputId":"a009c726-2ff6-4a1e-d1bd-0e89da32497d"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98babaf6e6c942cea9630bc1586c29ec","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/20302 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["a_tran_all = []\n","for out in tqdm(translator(a_dataset, batch_size=16), total=len(a_dataset)):\n","    a_tran_all.append(out)"]},{"cell_type":"code","execution_count":null,"id":"1b85e43c","metadata":{"id":"1b85e43c"},"outputs":[],"source":["a_tran_all = [a[0]['translation_text'] for a in a_tran_all]"]},{"cell_type":"code","execution_count":null,"id":"6c01004e","metadata":{"colab":{"referenced_widgets":["7f88d519b05342b0afba412b316571f3"]},"id":"6c01004e","outputId":"c8613687-5eef-4bb8-c9c3-a034a5dc8a55"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f88d519b05342b0afba412b316571f3","version_major":2,"version_minor":0},"text/plain":["Paragraphs:   0%|          | 0/35 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["i=0\n","for di in tqdm(range(len(data2['data'])),desc=\"Paragraphs\"):\n","  d = data2['data'][di]\n","  for p in d['paragraphs']:\n","    for qas in p['qas']:\n","      for ans in qas['answers']:\n","        ans['a_tran_temp'] = a_tran_all[i]\n","        i+=1"]},{"cell_type":"code","execution_count":null,"id":"89cad7da","metadata":{"colab":{"referenced_widgets":["94e28a6be7794907b6418b2ce9d16aac"]},"id":"89cad7da","outputId":"578e9f38-f5a3-4878-f43b-3eb0976f966d"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94e28a6be7794907b6418b2ce9d16aac","version_major":2,"version_minor":0},"text/plain":["Paragraphs:   0%|          | 0/35 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# for di in tqdm(range(len(data['data'])),desc=\"Paragraphs\"):\n","import string\n","import re\n","punc = '''!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~—।–'''\n","def get_a_tran(c_tran_with_ans, a_src, a_tran_temp):\n","  a_tran = a_src\n","  if a_src in c_tran_with_ans:\n","    a_tran = a_src\n","  elif len(word_tokenize(a_src))==1:\n","    clean_ans_text = a_src.translate(str.maketrans('', '', punc))\n","    if re.match('^[0-9]*$', clean_ans_text):\n","      a_tran = translate_digits(a_src)\n","  else:\n","    a_tran = a_tran_temp\n","    a_tran = translate_digits(a_tran)\n","  return a_tran\n","\n","\n","for di in tqdm(range(len(data2['data'])),desc=\"Paragraphs\"):\n","  d = data2['data'][di]\n","  for p in d['paragraphs']:\n","    # for qas in tqdm(p['qas'],desc=\"QuesAns\"):\n","    c_tran_sent_list = p['bangla_context']\n","    c_src_sent_list = nltk.sent_tokenize(p['context'])\n","    assert len(c_src_sent_list)==len(c_tran_sent_list)\n","\n","    for qas in p['qas']:\n","      q_tran = qas['q_tran']\n","      for ans in qas['answers']:\n","        a_ch_start_pos_src = ans['answer_start']\n","        a_src = ans['text']\n","        a_tran_temp = ans['a_tran_temp']\n","        index_c_tran_with_ans = get_index_c_tran_with_ans(c_src_sent_list, c_tran_sent_list, a_ch_start_pos_src)\n","        a_tran = get_a_tran(c_tran_sent_list[index_c_tran_with_ans], a_src,  a_tran_temp)\n","\n","        ans['a_tran'] = a_tran\n","        ans['index_c_tran_with_ans'] = index_c_tran_with_ans"]},{"cell_type":"code","execution_count":null,"id":"5f279163","metadata":{"id":"5f279163"},"outputs":[],"source":["# out_file = open(save_loc+'/'+'squad2_dev_temp_translated_cqa_tran'+'.json', \"w\")\n","# json.dump(data2, out_file, indent = 4) # save whole data replace parts later\n","# out_file.close()"]},{"cell_type":"markdown","source":["# **Translate All Plausible Answers**"],"metadata":{"id":"m7VzCA5jFRYI"},"id":"m7VzCA5jFRYI"},{"cell_type":"code","execution_count":null,"id":"ef99c8e0","metadata":{"colab":{"referenced_widgets":["abc2008c0bd34cb0a5599d3701d563b9"]},"id":"ef99c8e0","outputId":"ed8f6651-ce11-453c-b996-29ff0be28006"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"abc2008c0bd34cb0a5599d3701d563b9","version_major":2,"version_minor":0},"text/plain":["Paragraphs:   0%|          | 0/35 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["pa_src_all = []\n","for di in tqdm(range(0,len(data2['data'])),desc=\"Paragraphs\"):\n","  d = data2['data'][di]\n","  for p in d['paragraphs']:\n","    for qas in p['qas']:\n","        if 'plausible_answers' in qas:\n","          for pans in qas['plausible_answers']:\n","              pa_src = re.sub(r'(?<=\\d)[,\\.](?=\\d)','',pans['text'])\n","              pa_src_all.append(pa_src)"]},{"cell_type":"code","execution_count":null,"id":"cfe7feee","metadata":{"id":"cfe7feee"},"outputs":[],"source":["pa_dataset = ADataset(pa_src_all)"]},{"cell_type":"code","execution_count":null,"id":"548c788f","metadata":{"colab":{"referenced_widgets":["94b520936ce74aa5b769dab437c28038"]},"id":"548c788f","outputId":"78e42089-dfde-454f-d574-f0e25dbea54e"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94b520936ce74aa5b769dab437c28038","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5930 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["pa_tran_all = []\n","for out in tqdm(translator(pa_dataset, batch_size=16), total=len(pa_dataset)):\n","    pa_tran_all.append(out)"]},{"cell_type":"code","execution_count":null,"id":"290c491e","metadata":{"id":"290c491e"},"outputs":[],"source":["pa_tran_all = [pa[0]['translation_text'] for pa in pa_tran_all]"]},{"cell_type":"code","execution_count":null,"id":"b9fce738","metadata":{"colab":{"referenced_widgets":["64fa900b290b4ea986ac775200b7ca17"]},"id":"b9fce738","outputId":"46f16c32-450b-4696-f3c0-e2ac0c2cdb4a"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64fa900b290b4ea986ac775200b7ca17","version_major":2,"version_minor":0},"text/plain":["Paragraphs:   0%|          | 0/35 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["i=0\n","for di in tqdm(range(len(data2['data'])),desc=\"Paragraphs\"):\n","  d = data2['data'][di]\n","  for p in d['paragraphs']:\n","      for qas in p['qas']:\n","        if 'plausible_answers' in qas:\n","          for pans in qas['plausible_answers']:\n","            pans['a_tran_temp'] = pa_tran_all[i]\n","            i+=1"]},{"cell_type":"code","execution_count":null,"id":"abdb2c98","metadata":{"colab":{"referenced_widgets":["ebb1c3e18bc04909934f90688967f74a"]},"id":"abdb2c98","outputId":"c81c3ce1-9a1f-49c0-e3b1-53ebc56d6a7e"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ebb1c3e18bc04909934f90688967f74a","version_major":2,"version_minor":0},"text/plain":["Paragraphs:   0%|          | 0/35 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# for di in tqdm(range(len(data['data'])),desc=\"Paragraphs\"):\n","for di in tqdm(range(len(data2['data'])),desc=\"Paragraphs\"):\n","  d = data2['data'][di]\n","  for p in d['paragraphs']:\n","    c_tran_sent_list = p['bangla_context']\n","    c_src_sent_list = nltk.sent_tokenize(p['context'])\n","    assert len(c_src_sent_list)==len(c_tran_sent_list)\n","    for qas in p['qas']:\n","        if 'plausible_answers' in qas:\n","          for pans in qas['plausible_answers']:\n","              a_ch_start_pos_src = pans['answer_start']\n","              a_src = pans['text']\n","              a_tran_temp = pans['a_tran_temp']\n","              index_c_tran_with_ans = get_index_c_tran_with_ans(c_src_sent_list, c_tran_sent_list, a_ch_start_pos_src)\n","              a_tran = get_a_tran(c_tran_sent_list[index_c_tran_with_ans], a_src, a_tran_temp)\n","\n","              pans['a_tran'] = a_tran\n","              pans['index_c_tran_with_ans'] = index_c_tran_with_ans"]},{"cell_type":"code","execution_count":null,"id":"b8f7a82c","metadata":{"id":"b8f7a82c"},"outputs":[],"source":["out_file = open(save_loc+'/'+'squad2_temp_translated'+'.json', \"w\")\n","json.dump(data2, out_file, indent = 4) # save whole data replace parts later\n","out_file.close()"]},{"cell_type":"code","execution_count":null,"id":"b0a06d7f","metadata":{"colab":{"referenced_widgets":["c57a2fa8498b4aceb03e890aa6d994e1"]},"id":"b0a06d7f","outputId":"e7f6848e-d589-4916-b30d-5ca8409f2133"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c57a2fa8498b4aceb03e890aa6d994e1","version_major":2,"version_minor":0},"text/plain":["Paragraphs:   0%|          | 0/442 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'qas': [{'question': 'When did Beyonce start becoming popular?', 'id': '56be85543aeaaa14008c9063', 'answers': [{'text': 'in the late 1990s', 'answer_start': 269, 'a_tran_temp': '১৯৯০ এর দশকের শেষের দিকে', 'a_tran': '১৯৯০ এর দশকের শেষের দিকে', 'index_c_tran_with_ans': 1}], 'is_impossible': False, 'q_tran': 'বিয়োনস কবে থেকে জনপ্রিয় হতে শুরু করে?'}, {'question': 'What areas did Beyonce compete in when she was growing up?', 'id': '56be85543aeaaa14008c9065', 'answers': [{'text': 'singing and dancing', 'answer_start': 207, 'a_tran_temp': 'গান আর নাচ', 'a_tran': 'গান আর নাচ', 'index_c_tran_with_ans': 1}], 'is_impossible': False, 'q_tran': 'বিয়োনস যখন বড় হচ্ছিল তখন কোন কোন ক্ষেত্রে প্রতিযোগিতা করতো?'}, {'question': \"When did Beyonce leave Destiny's Child and become a solo singer?\", 'id': '56be85543aeaaa14008c9066', 'answers': [{'text': '2003', 'answer_start': 526, 'a_tran_temp': '২০০৩', 'a_tran': '2003', 'index_c_tran_with_ans': 3}], 'is_impossible': False, 'q_tran': 'বেয়নস কখন ডেসটিনিস চাইল্ড ছেড়ে একক গায়ক হয়েছিলেন?'}, {'question': 'In what city and state did Beyonce  grow up? ', 'id': '56bf6b0f3aeaaa14008c9601', 'answers': [{'text': 'Houston, Texas', 'answer_start': 166, 'a_tran_temp': 'হিউস্টন, টেক্সাস', 'a_tran': 'হিউস্টন, টেক্সাস', 'index_c_tran_with_ans': 1}], 'is_impossible': False, 'q_tran': 'কোন শহরে এবং রাজ্যে বিয়োনস বড় হয়েছিল?'}, {'question': 'In which decade did Beyonce become famous?', 'id': '56bf6b0f3aeaaa14008c9602', 'answers': [{'text': 'late 1990s', 'answer_start': 276, 'a_tran_temp': 'নব্বইয়ের দশকের শেষভাগ', 'a_tran': 'নব্বইয়ের দশকের শেষভাগ', 'index_c_tran_with_ans': 1}], 'is_impossible': False, 'q_tran': 'কোন দশকে বিয়োনস বিখ্যাত হয়ে ওঠে?'}, {'question': 'In what R&B group was she the lead singer?', 'id': '56bf6b0f3aeaaa14008c9603', 'answers': [{'text': \"Destiny's Child\", 'answer_start': 320, 'a_tran_temp': 'ডেসটিনির শিশু', 'a_tran': 'ডেসটিনির শিশু', 'index_c_tran_with_ans': 1}], 'is_impossible': False, 'q_tran': 'কোন আর অ্যান্ড বি গ্রুপে সে ছিল প্রধান গায়ক?'}, {'question': 'What album made her a worldwide known artist?', 'id': '56bf6b0f3aeaaa14008c9604', 'answers': [{'text': 'Dangerously in Love', 'answer_start': 505, 'a_tran_temp': 'বিপজ্জনকভাবে প্রেমে পড়ে', 'a_tran': 'Dangerously in Love', 'index_c_tran_with_ans': 3}], 'is_impossible': False, 'q_tran': 'কোন অ্যালবাম তাকে বিশ্বব্যাপী পরিচিত শিল্পী করেছে?'}, {'question': \"Who managed the Destiny's Child group?\", 'id': '56bf6b0f3aeaaa14008c9605', 'answers': [{'text': 'Mathew Knowles', 'answer_start': 360, 'a_tran_temp': 'ম্যাথিউ নোলস', 'a_tran': 'ম্যাথিউ নোলস', 'index_c_tran_with_ans': 2}], 'is_impossible': False, 'q_tran': \"ডেসটিনি'স চাইল্ড গ্রুপের ম্যানেজার কে?\"}, {'question': 'When did Beyoncé rise to fame?', 'id': '56d43c5f2ccc5a1400d830a9', 'answers': [{'text': 'late 1990s', 'answer_start': 276, 'a_tran_temp': 'নব্বইয়ের দশকের শেষভাগ', 'a_tran': 'নব্বইয়ের দশকের শেষভাগ', 'index_c_tran_with_ans': 1}], 'is_impossible': False, 'q_tran': 'বিয়োনসে কবে খ্যাতি অর্জন করেন?'}, {'question': \"What role did Beyoncé have in Destiny's Child?\", 'id': '56d43c5f2ccc5a1400d830aa', 'answers': [{'text': 'lead singer', 'answer_start': 290, 'a_tran_temp': 'প্রধান গায়ক', 'a_tran': 'প্রধান গায়ক', 'index_c_tran_with_ans': 1}], 'is_impossible': False, 'q_tran': \"ডেসটিনি'স চাইল্ডে বিয়োনসের কি ভূমিকা ছিল?\"}, {'question': 'What was the first album Beyoncé released as a solo artist?', 'id': '56d43c5f2ccc5a1400d830ab', 'answers': [{'text': 'Dangerously in Love', 'answer_start': 505, 'a_tran_temp': 'বিপজ্জনকভাবে প্রেমে পড়ে', 'a_tran': 'Dangerously in Love', 'index_c_tran_with_ans': 3}], 'is_impossible': False, 'q_tran': 'বেয়নসে একক শিল্পী হিসেবে প্রথম কোন অ্যালবাম প্রকাশ করেছিলেন?'}, {'question': 'When did Beyoncé release Dangerously in Love?', 'id': '56d43c5f2ccc5a1400d830ac', 'answers': [{'text': '2003', 'answer_start': 526, 'a_tran_temp': '২০০৩', 'a_tran': '2003', 'index_c_tran_with_ans': 3}], 'is_impossible': False, 'q_tran': 'বিয়োনসে কখন Dangerously in Love প্রকাশ করেছে?'}, {'question': 'How many Grammy awards did Beyoncé win for her first solo album?', 'id': '56d43c5f2ccc5a1400d830ad', 'answers': [{'text': 'five', 'answer_start': 590, 'a_tran_temp': 'পাঁচ', 'a_tran': 'five', 'index_c_tran_with_ans': 3}], 'is_impossible': False, 'q_tran': 'বেয়নসে তার প্রথম একক অ্যালবাম জন্য কত গ্র্যামি পুরস্কার জিতেছে?'}, {'question': \"What was Beyoncé's role in Destiny's Child?\", 'id': '56d43ce42ccc5a1400d830b4', 'answers': [{'text': 'lead singer', 'answer_start': 290, 'a_tran_temp': 'প্রধান গায়ক', 'a_tran': 'প্রধান গায়ক', 'index_c_tran_with_ans': 1}], 'is_impossible': False, 'q_tran': \"ডেসটিনি'স চাইল্ডে বিয়োনসের ভূমিকা কী ছিল?\"}, {'question': \"What was the name of Beyoncé's first solo album?\", 'id': '56d43ce42ccc5a1400d830b5', 'answers': [{'text': 'Dangerously in Love', 'answer_start': 505, 'a_tran_temp': 'বিপজ্জনকভাবে প্রেমে পড়ে', 'a_tran': 'Dangerously in Love', 'index_c_tran_with_ans': 3}], 'is_impossible': False, 'q_tran': 'বিয়োনসের প্রথম একক অ্যালবামের নাম কি ছিল?'}], 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'bangla_context': ['বিয়োনসে গিসেল নলস-কার্টার (/biːˈjɒnseɪ/ bee-YON-say) (জন্ম ৪ সেপ্টেম্বর, ১৯৮১) একজন আমেরিকান গায়ক, গীতিকার, রেকর্ড প্রযোজক এবং অভিনেত্রী।', \"টেক্সাসের হিউস্টনে জন্মগ্রহণ ও বেড়ে ওঠা, তিনি ছোটবেলায় বিভিন্ন গায়িকা এবং নৃত্য প্রতিযোগিতায় অংশ নিয়েছিলেন এবং ১৯৯০ এর দশকের শেষের দিকে আর অ্যান্ড বি গার্ল গ্রুপ ডেসটিনি'স চাইল্ডের প্রধান গায়ক হিসাবে খ্যাতি অর্জন করেছিলেন।\", 'তার বাবা ম্যাথিউ নলস পরিচালিত এই গ্রুপটি বিশ্বের অন্যতম সেরা বিক্রিত মেয়েদের গ্রুপে পরিণত হয়।', 'তাদের বিরতিতে বেয়োনসের প্রথম অ্যালবাম, Dangerously in Love (2003), প্রকাশিত হয়, যা তাকে বিশ্বব্যাপী একক শিল্পী হিসাবে প্রতিষ্ঠিত করে, পাঁচটি গ্র্যামি পুরস্কার অর্জন করে এবং বিলবোর্ড হট ১০০-এর এক নম্বর একক \"Crazy in Love\" এবং \"Baby Boy\" বৈশিষ্ট্যযুক্ত।']}\n"]}],"source":["for di in tqdm(range(len(data2['data'])),desc=\"Paragraphs\"):\n","  d = data2['data'][di]\n","  for p in d['paragraphs']:\n","    print(p)\n","    break;\n","  break;\n"]},{"cell_type":"markdown","source":["# **Answer span correction and alignment**"],"metadata":{"id":"iiOgzDZLGdSG"},"id":"iiOgzDZLGdSG"},{"cell_type":"code","source":["!wget \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.bn.300.bin.gz\"\n","!gunzip './cc.bn.300.bin.gz'"],"metadata":{"id":"f_iY_n1fFlWN"},"id":"f_iY_n1fFlWN","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"a71a5681","metadata":{"id":"a71a5681","outputId":"6b906da8-922a-4c8b-dbde-ec89ad8fea06"},"outputs":[{"name":"stderr","output_type":"stream","text":["Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"]}],"source":["import io\n","# from gensim.models import KeyedVectors\n","import fasttext\n","fasttext_model = fasttext.load_model(\"./cc.bn.300.bin\")"]},{"cell_type":"code","execution_count":null,"id":"d09e6007","metadata":{"id":"d09e6007","outputId":"242913c1-cdce-4497-b1a4-2c8b95f92957"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-12-30 14:17:31.587949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-12-30 14:17:31.677784: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2022-12-30 14:17:31.693878: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2022-12-30 14:17:31.960937: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64\n","2022-12-30 14:17:31.960971: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64\n","2022-12-30 14:17:31.960973: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"]}],"source":["from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer"]},{"cell_type":"code","execution_count":null,"id":"527f2562","metadata":{"id":"527f2562"},"outputs":[],"source":["bn_to_en_tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5_nmt_bn_en\")\n","bn_to_en_model = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/banglat5_nmt_bn_en\")"]},{"cell_type":"code","execution_count":null,"id":"93d849c1","metadata":{"id":"93d849c1"},"outputs":[],"source":["bn_to_en_translator = pipeline('translation', model=bn_to_en_model, tokenizer=bn_to_en_tokenizer, device=\"cuda:0\")"]},{"cell_type":"code","execution_count":null,"id":"36e866be","metadata":{"id":"36e866be"},"outputs":[],"source":["import string\n","punc = '''!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~—।–'''\n","uw = 0.001\n","punc_embed = {}\n","for p in punc:\n","  punc_embed[p] = np.random.uniform(-uw,uw,300)\n","\n","sp_punc1 = np.random.uniform(-uw,uw,300)\n","sp_punc2 = np.random.uniform(-uw,uw,300)\n","\n","def get_embedding(word):\n","    word = word.strip(\"@\")\n","    # if word==\"``\" or word==\"''\" or word==\"‘\" or word==\"’\":\n","    #   word = '\"'\n","    if word==\"....\" or word==\"...\" or word==\"..\":\n","        return sp_punc1\n","    if word==\"()\":\n","        return sp_punc2\n","    if len(word)==1 and word in punc:\n","        return punc_embed[word]\n","    word = word.translate(str.maketrans('', '', punc))\n","    # if len(word)>3 and word not in ['ইউনিভার্সিটি','দুটি','সিটি'] and re.search(r'টি$', word):\n","    #     word = word[:-2]\n","    # word = re.sub(\"([০-৯]+)([অ-য়]+)\",r\"\\1\", word)\n","    word_vec = fasttext_model.get_word_vector(word)\n","    return word_vec"]},{"cell_type":"code","execution_count":null,"id":"e81535a9","metadata":{"id":"e81535a9"},"outputs":[],"source":["from utoken import utokenize\n","from utoken import detokenize\n","tok = utokenize.Tokenizer(lang_code='ben')\n","detok = detokenize.Detokenizer(lang_code='ben')\n","\n","# print(tok.utokenize_string(\"Dont worry!\"))\n"]},{"cell_type":"code","execution_count":null,"id":"dbe5f785","metadata":{"id":"dbe5f785"},"outputs":[],"source":["import json\n","\n","f = open('./squad2_data/squad2_temp_translated.json', 'r')\n","data = json.load(f)\n","f.close()"]},{"cell_type":"code","execution_count":null,"id":"b4ab12d3","metadata":{"colab":{"referenced_widgets":["bd1d723e14b643ca8f79ae490546a151"]},"id":"b4ab12d3","outputId":"260b1204-bae1-430b-e2de-a0e142d5f834"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd1d723e14b643ca8f79ae490546a151","version_major":2,"version_minor":0},"text/plain":["Paragraphs:   0%|          | 0/35 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import re\n","regex_year = re.compile(\"([০-৯]+)-([০-৯]+)\")\n","digits_dict = {\"0\":'০', \"1\": '১', \"2\": '২', \"3\": '৩', \"4\": '৪', \"5\": '৫', \"6\": '৬', \"7\": '৭', \"8\": '৮', \"9\": '৯'}\n","def digit_to_bn(digit):\n","    return digits_dict[digit.group()]\n","for di in tqdm(range(len(data['data'])),desc=\"Paragraphs\"):\n","    d = data['data'][di]\n","    for p in d['paragraphs']:\n","        new_c_tran = []\n","        for c_tran in p['bangla_context']:\n","            c_tran = re.sub(\"[0-9]\",digit_to_bn, c_tran)\n","            c_tran = re.sub(\"([০-৯]+)(th)\",r\"\\1তম\", c_tran)\n","            c_tran = re.sub(\"([০-৯]+)([অ-য়]+)\",r\"\\1 \\2\", c_tran)\n","            c_tran = re.sub(\"([১-২][০-৯][০-৯][০-৯])([১-২]?[০-৯]?[০-৯][০-৯])\",r\"\\1-\\2\", c_tran)\n","            new_c_tran.append(c_tran)\n","        p['bangla_context'] = new_c_tran\n","\n","        new_c_tran = []\n","        # new_c_tran.append(p['bangla_context'][0])\n","        j=0\n","        for i,c_tran in enumerate(p['bangla_context']):\n","            if i!=0 and (new_c_tran[-1][-1]=='.' or len(c_tran)<30):\n","                new_c_tran[-1]+=(\" \"+c_tran)\n","                for qas in p['qas']:\n","                    for ans in qas['answers']:\n","                        if ans['index_c_tran_with_ans']>=(i-j):\n","                            ans['index_c_tran_with_ans']-=1\n","                j+=1\n","            else:\n","                new_c_tran.append(c_tran)                \n","        p['bangla_context'] = new_c_tran\n","\n","        for qas in p['qas']:\n","            qas['q_tran'] = re.sub(\"[0-9]\",digit_to_bn, qas['q_tran'])\n","            qas['q_tran'] = re.sub(\"([০-৯]+)(th)\",r\"\\1তম\", qas['q_tran'])\n","            qas['q_tran'] = re.sub(\"([০-৯]+)([অ-য়]+)\",r\"\\1 \\2\", qas['q_tran'])\n","            qas['q_tran'] = re.sub(\"([১-২][০-৯][০-৯][০-৯])([১-২]?[০-৯]?[০-৯][০-৯])\",r\"\\1-\\2\", qas['q_tran'])\n","            for ans in qas['answers']:\n","                ans['a_tran'] = re.sub(\"[0-9]\",digit_to_bn, ans['a_tran'])\n","                ans['a_tran'] = re.sub(\"([০-৯]+)(th)\",r\"\\1তম\", ans['a_tran'])\n","                ans['a_tran'] = re.sub(\"([০-৯]+)([অ-য়]+)\",r\"\\1 \\2\", ans['a_tran'])\n","                ans['a_tran'] = re.sub(\"([১-২][০-৯][০-৯][০-৯])([১-২]?[০-৯]?[০-৯][০-৯])\",r\"\\1-\\2\", ans['a_tran'])\n"]},{"cell_type":"code","execution_count":null,"id":"dc230c5e","metadata":{"colab":{"referenced_widgets":["11d70d8091d346189e5773abe802fa1d"]},"id":"dc230c5e","outputId":"101f5de1-bef9-4b15-8e9f-ee053135d8f0"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"11d70d8091d346189e5773abe802fa1d","version_major":2,"version_minor":0},"text/plain":["Paragraphs:   0%|          | 0/35 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["digits_en = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n","digits_bn = ['০', '১', '২', '৩', '৪', '৫', '৬', '৭', '৮', '৯']\n","for di in tqdm(range(len(data['data'])),desc=\"Paragraphs\"):\n","    d = data['data'][di]\n","    for p in d['paragraphs']:\n","        c_tran_sent_list = p['bangla_context']\n","        c_tran_final = \"\"\n","        for c_tran_sent in c_tran_sent_list:\n","            c_tran_final += (c_tran_sent + \" \")\n","        if any(n in c_tran_final for n in digits_en):\n","            print('c_tran: ', c_tran_final)\n","        for qas in p['qas']:\n","            if any(n in qas['q_tran'] for n in digits_en):\n","                print('q_tran: ', qas['q_tran'])\n","            for ans in qas['answers']:\n","                if any(n in ans['a_tran'] for n in digits_en):\n","                    print('a_tran: ', ans['a_tran'])"]},{"cell_type":"code","execution_count":null,"id":"c932a15e","metadata":{"id":"c932a15e"},"outputs":[],"source":["def get_answer_start(index_c_tran_with_ans, a_tran, c_tran, debug=False):\n","  c_tran_with_ans = c_tran[index_c_tran_with_ans]\n","  c_tran_with_ans_token = tok.utokenize_string(c_tran_with_ans).split(\" \")\n","#   a_tran = a_tran.translate(str.maketrans('', '', '''`'\",।|'''))\n","  a_tran_token = tok.utokenize_string(a_tran).split(\" \")\n","#   a_tran_token = [token for token in a_tran_token if token!='']\n","  if debug:\n","      print(c_tran_with_ans_token)  \n","      print(a_tran_token)\n","  if len(a_tran_token)==0:\n","    return a_tran, 0, 0 \n","#   if debug:\n","#     print(c_tran_with_ans_token)\n","#     print(a_tran_token)\n","  ans_vec = np.array([np.array(get_embedding(a_word)) for a_word in a_tran_token])\n","  #print(ans_vec)\n","  ans_norm = np.apply_along_axis(np.linalg.norm,1,ans_vec)\n","  # print(ans_norm)\n","  lengths = [-1, 0, 1]\n","  sim_lists = []\n","  for length in lengths:\n","    if len(a_tran_token)+length>=len(c_tran_with_ans_token):\n","        sim_lists.append(np.array([-1]))\n","        continue\n","    sim_mat = []\n","    for i in range(len(a_tran_token)+length):\n","        c_word = c_tran_with_ans_token[i]\n","        cw_vec = np.array(get_embedding(c_word))\n","        cw_norm = np.linalg.norm(cw_vec)\n","        sim_measure = (np.dot(ans_vec,cw_vec)/ans_norm)/cw_norm\n","        sim_mat.append(sim_measure)\n","\n","    sim_mat = np.array(sim_mat)\n","    # print(sim_mat.shape)\n","    sim_list = []\n","\n","    for i in range(len(a_tran_token)+length,len(c_tran_with_ans_token)):\n","        c_word = c_tran_with_ans_token[i]\n","        cw_vec = np.array(get_embedding(c_word))\n","        cw_norm = np.linalg.norm(cw_vec)\n","        sim_measures = (np.dot(ans_vec,cw_vec)/ans_norm)/cw_norm\n","        if(sim_mat.shape[0]==0):\n","            sim_mat = np.array([sim_measures])\n","        else:\n","            sim_mat = np.append(sim_mat,np.array([sim_measures]),0)\n","        sim_mat_cal = sim_mat\n","        \n","        similarity = 0\n","        for j in range(len(a_tran_token)):\n","            max_index = np.unravel_index(sim_mat_cal.argmax(),sim_mat_cal.shape)\n","            similarity+= sim_mat_cal[max_index]\n","            sim_mat_cal = np.delete(sim_mat_cal,max_index[0],0)\n","            sim_mat_cal = np.delete(sim_mat_cal,max_index[1],1)\n","\n","        sim_list.append(similarity)\n","        sim_mat = np.delete(sim_mat,0,0)\n","\n","    sim_list = np.divide(sim_list,len(a_tran_token))\n","    # if debug:\n","    #     print(sim_list)\n","    sim_lists.append(sim_list)\n","\n","\n","\n","\n","  if(max(sim_lists[2]) - max(sim_lists[0]) >= 0.08 and max(sim_lists[2]) - max(sim_lists[1]) >= 0.03):\n","    # print(c_tran_with_ans_token[sim_lists[2].argmax()])\n","    align_score = max(sim_lists[2])\n","    best_ans_start = sim_lists[2].argmax()\n","    best_ans_length = len(a_tran_token)+2#lengths[2]\n","    \n","    if debug:\n","        print('best in equal+2')\n","  elif(max(sim_lists[1]) - max(sim_lists[0]) >= 0.06):\n","    # print(c_tran_with_ans_token[sim_lists[1].argmax()])\n","    align_score = max(sim_lists[1])\n","    best_ans_start = sim_lists[1].argmax()\n","    best_ans_length = len(a_tran_token)+1#lengths[1]\n","\n","    if debug:\n","        print('best in equal+1')\n","  else:\n","    # print(c_tran_with_ans_token[sim_lists[0].argmax()])\n","    align_score = max(sim_lists[0])\n","    best_ans_start = sim_lists[0].argmax()\n","    best_ans_length = len(a_tran_token)#lengths[0]\n","    \n","    if debug:\n","        print('best in equal')\n","\n","  \n","  if best_ans_start+best_ans_length>len(c_tran_with_ans_token):\n","      best_ans_length=len(c_tran_with_ans_token)-best_ans_start+1\n","\n","  a_tran_final = detok.detokenize_string(\" \".join(c_tran_with_ans_token[best_ans_start:best_ans_start+best_ans_length]))\n","  a_tran_final = re.sub(\"([০-৯]+)([অ-য়]+)\",r\"\\1 \\2\", a_tran_final)\n","\n","  ans_start_ref = c_tran_with_ans.find(a_tran_final)\n","  if debug:\n","      print(a_tran_final)  \n","\n","  ans_start_final = 0\n","  for i in range(len(c_tran)):\n","    c_tran_sent = c_tran[i]\n","    if i == index_c_tran_with_ans:\n","      ans_start_final+= ans_start_ref  \n","    if i < index_c_tran_with_ans:        \n","      ans_start_final += len(c_tran_sent)+1\n","      \n","  # print(c_tran_final)\n","  # print(ans_start_final)\n","  return a_tran_final, ans_start_final,align_score"]},{"cell_type":"code","execution_count":null,"id":"ee137624","metadata":{"id":"ee137624"},"outputs":[],"source":["import re\n","align_punc = '''!\"#&'*,./:;<=>?@\\^_`-|~—।–'''\n","align_punc2 = '''!#&'*,/:;<=>?@\\^_`-|~—।–'''\n","\n","regexp = re.compile(r'[A-z]')"]},{"cell_type":"code","execution_count":null,"id":"add92a24","metadata":{"id":"add92a24"},"outputs":[],"source":["regexp_bn = re.compile(r'[অ-য়]')\n","def translate_bn_en(text):\n","    if regexp_bn.search(text):\n","        return bn_to_en_translator(text)[0]['translation_text']\n","    else:\n","        return text"]},{"cell_type":"code","execution_count":null,"id":"d4bacf11","metadata":{"id":"d4bacf11"},"outputs":[],"source":["def get_back_trans_answer_start(c_tran_with_ans_token, a_tran_token, debug=False):\n","  if len(a_tran_token)==0:\n","    return a_tran, 0, 0 \n","\n","  ans_vec = np.array([np.array(get_embedding(a_word)) for a_word in a_tran_token])\n","  #print(ans_vec)\n","  ans_norm = np.apply_along_axis(np.linalg.norm,1,ans_vec)\n","  # print(ans_norm)\n","  lengths = [-1, 0, 1]\n","  sim_lists = []\n","  for length in lengths:\n","    if len(a_tran_token)+length>=len(c_tran_with_ans_token):\n","        sim_lists.append(np.array([-1]))\n","        continue\n","    sim_mat = []\n","    for i in range(len(a_tran_token)+length):\n","        c_word = c_tran_with_ans_token[i]\n","        cw_vec = np.array(get_embedding(c_word))\n","        cw_norm = np.linalg.norm(cw_vec)\n","        sim_measure = (np.dot(ans_vec,cw_vec)/ans_norm)/cw_norm\n","        sim_mat.append(sim_measure)\n","\n","    sim_mat = np.array(sim_mat)\n","    # print(sim_mat.shape)\n","    sim_list = []\n","\n","    for i in range(len(a_tran_token)+length,len(c_tran_with_ans_token)):\n","        c_word = c_tran_with_ans_token[i]\n","        cw_vec = np.array(get_embedding(c_word))\n","        cw_norm = np.linalg.norm(cw_vec)\n","        sim_measures = (np.dot(ans_vec,cw_vec)/ans_norm)/cw_norm\n","        if(sim_mat.shape[0]==0):\n","            sim_mat = np.array([sim_measures])\n","        else:\n","            sim_mat = np.append(sim_mat,np.array([sim_measures]),0)\n","        sim_mat_cal = sim_mat\n","        \n","        similarity = 0\n","        for j in range(len(a_tran_token)):\n","            max_index = np.unravel_index(sim_mat_cal.argmax(),sim_mat_cal.shape)\n","            similarity+= sim_mat_cal[max_index]\n","            sim_mat_cal = np.delete(sim_mat_cal,max_index[0],0)\n","            sim_mat_cal = np.delete(sim_mat_cal,max_index[1],1)\n","\n","        sim_list.append(similarity)\n","        sim_mat = np.delete(sim_mat,0,0)\n","\n","    sim_list = np.divide(sim_list,len(a_tran_token))\n","    # if debug:\n","    #     print(sim_list)\n","    sim_lists.append(sim_list)\n","\n","  if(max(sim_lists[1]) - max(sim_lists[0]) >= 0.06):\n","    # print(c_tran_with_ans_token[sim_lists[1].argmax()])\n","    align_score = max(sim_lists[1])\n","    best_ans_start = sim_lists[1].argmax()\n","    best_ans_length = len(a_tran_token)+1#lengths[1]\n","\n","    if debug:\n","        print('best in equal+1')\n","  else:\n","    # print(c_tran_with_ans_token[sim_lists[0].argmax()])\n","    align_score = max(sim_lists[0])\n","    best_ans_start = sim_lists[0].argmax()\n","    best_ans_length = len(a_tran_token)#lengths[0]\n","    \n","    if debug:\n","        print('best in equal')\n","  \n","  if best_ans_start+best_ans_length>len(c_tran_with_ans_token):\n","      best_ans_length=len(c_tran_with_ans_token)-best_ans_start+1\n","      \n","  return best_ans_start ,align_score"]},{"cell_type":"code","execution_count":null,"id":"add1e492","metadata":{"id":"add1e492"},"outputs":[],"source":["data['data'][0]['paragraphs'][0]"]},{"cell_type":"code","execution_count":null,"id":"638648e3","metadata":{"colab":{"referenced_widgets":["d30d500b0aa84d8d93ca8d25e58650b6"]},"id":"638648e3","outputId":"3085fcc6-e6ba-42dd-9cab-f7e1bb9934a1"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d30d500b0aa84d8d93ca8d25e58650b6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/35 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_42112/1599377179.py:41: RuntimeWarning: invalid value encountered in divide\n","  sim_measures = (np.dot(ans_vec,cw_vec)/ans_norm)/cw_norm\n","/tmp/ipykernel_42112/1599377179.py:30: RuntimeWarning: invalid value encountered in divide\n","  sim_measure = (np.dot(ans_vec,cw_vec)/ans_norm)/cw_norm\n"]}],"source":["import copy\n","\n","tran_data = data\n","c_tran_tokens = []\n","c_tran_tokens_len = [len(c_tran_tokens)]\n","a_tran_tokens = []\n","a_tran_tokens_len = [len(a_tran_tokens)]\n","\n","for di in tqdm(range(len(tran_data['data']))):\n","  d = tran_data['data'][di]\n","  for p in d['paragraphs']:\n","    c_tran_sent_list = copy.deepcopy(p['bangla_context'])\n","    c_tran_final = \" \".join(copy.deepcopy(p['bangla_context']))\n","    p['bangla_context_list'] = c_tran_sent_list\n","    p['bangla_context'] = c_tran_final\n","    for qas in p['qas']:\n","      for ans in qas['answers']:\n","        a_tran = ans['a_tran']\n","        index_c_tran_with_ans = ans['index_c_tran_with_ans']\n","        a_src = ans['text']\n","        # print('banga question -> ',qas['q_tran'])\n","        # print('translated bangla answer -> ', a_tran)\n","        # print('english answer -> ', a_src)\n","        if len(a_tran)>1 and a_tran[0] in align_punc2:\n","            a_tran = a_tran[1:]\n","        if len(a_tran)>1 and a_tran[-1] in align_punc2:\n","            a_tran = a_tran[:-1]\n","        a_tran_final, ans_start_final, align_score = get_answer_start(index_c_tran_with_ans, a_tran, c_tran_sent_list,debug=False)\n","        \n","        if regexp.search(a_tran) or align_score<0.6: # or  \"'\" in a_tran_final or (a_tran_final[0] in align_punc or a_tran_final[-1] in align_punc):\n","            c_tran_with_ans_token = tok.utokenize_string(c_tran_sent_list[index_c_tran_with_ans]).split(\" \")\n","            c_tran_tokens.extend(c_tran_with_ans_token)\n","            c_tran_tokens_len.extend([len(c_tran_tokens)])\n","\n","            a_tran_token = tok.utokenize_string(a_tran).split(\" \")\n","            a_tran_tokens.extend(a_tran_token)\n","            a_tran_tokens_len.extend([len(a_tran_tokens)])\n","\n","        ans['a_tran_temp'] = a_tran\n","        ans['a_tran'] = a_tran_final\n","        ans['a_tran_start'] = ans_start_final\n","        ans['align_score'] = float(align_score)"]},{"cell_type":"code","execution_count":null,"id":"4553785d","metadata":{"id":"4553785d"},"outputs":[],"source":["import torch\n","class TextDataset(torch.utils.data.Dataset):\n","    def __init__(self, text):\n","        self.text = text\n","\n","    def __getitem__(self, idx):\n","        return self.text[idx].strip(\"@\")\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","\n","# convert our tokenized data into a torch Dataset\n","context_dataset = TextDataset(c_tran_tokens)\n","ans_dataset = TextDataset(a_tran_tokens)"]},{"cell_type":"code","execution_count":null,"id":"3ee76a38","metadata":{"colab":{"referenced_widgets":["a54ec3d4879146da83e7ef611f5b822f"]},"id":"3ee76a38","outputId":"afcf3679-e30f-4f2d-b62b-098b2799d701"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a54ec3d4879146da83e7ef611f5b822f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/200390 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["c_back_tran_tokens = []\n","for out in tqdm(bn_to_en_translator(context_dataset, batch_size=32), total=len(context_dataset)):\n","    c_back_tran_tokens.append(out)"]},{"cell_type":"code","execution_count":null,"id":"5ba26c46","metadata":{"colab":{"referenced_widgets":["5dfb1fbcc14e44af9a32651091137f77"]},"id":"5ba26c46","outputId":"5cf7cd72-2f02-4356-ac1c-fdb354e32751"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5dfb1fbcc14e44af9a32651091137f77","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/12835 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["ans_back_tran_tokens = []\n","for out in tqdm(bn_to_en_translator(ans_dataset, batch_size=32), total=len(ans_dataset)):\n","    ans_back_tran_tokens.append(out)"]},{"cell_type":"code","execution_count":null,"id":"e9e15106","metadata":{"id":"e9e15106"},"outputs":[],"source":["c_back_tran_tokens = [t[0]['translation_text'] for t in c_back_tran_tokens]"]},{"cell_type":"code","execution_count":null,"id":"fdabf9ae","metadata":{"id":"fdabf9ae"},"outputs":[],"source":["ans_back_tran_tokens = [t[0]['translation_text'] for t in ans_back_tran_tokens]"]},{"cell_type":"code","execution_count":null,"id":"9fd81dae","metadata":{"colab":{"referenced_widgets":["a2c20be781d049708110393d918df1a8"]},"id":"9fd81dae","outputId":"19573771-00dc-4d3a-aeb1-49b6967bb031"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2c20be781d049708110393d918df1a8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/35 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_42112/1599377179.py:41: RuntimeWarning: invalid value encountered in divide\n","  sim_measures = (np.dot(ans_vec,cw_vec)/ans_norm)/cw_norm\n","/tmp/ipykernel_42112/1599377179.py:30: RuntimeWarning: invalid value encountered in divide\n","  sim_measure = (np.dot(ans_vec,cw_vec)/ans_norm)/cw_norm\n","/tmp/ipykernel_42112/2944554396.py:31: RuntimeWarning: invalid value encountered in divide\n","  sim_measures = (np.dot(ans_vec,cw_vec)/ans_norm)/cw_norm\n","/tmp/ipykernel_42112/2944554396.py:20: RuntimeWarning: invalid value encountered in divide\n","  sim_measure = (np.dot(ans_vec,cw_vec)/ans_norm)/cw_norm\n"]}],"source":["ij=1\n","for di in tqdm(range(len(tran_data['data']))):\n","  d = tran_data['data'][di]\n","  for p in d['paragraphs']:\n","#     c_tran_sent_list = p['bangla_context']\n","#     c_tran_final = \" \".join(c_tran_sent_list)\n","#     p['bangla_context_list'] = c_tran_sent_list\n","#     p['bangla_context'] = c_tran_final\n","    c_tran_sent_list = p['bangla_context_list']\n","    for qas in p['qas']:\n","      for ans in qas['answers']:\n","        a_tran = ans['a_tran_temp']\n","        index_c_tran_with_ans = ans['index_c_tran_with_ans']\n","        a_src = ans['text']\n","        # print('banga question -> ',qas['q_tran'])\n","        # print('translated bangla answer -> ', a_tran)\n","        # print('english answer -> ', a_src)\n","        if len(a_tran)>1 and a_tran[0] in align_punc2:\n","            a_tran = a_tran[1:]\n","        if len(a_tran)>1 and a_tran[-1] in align_punc2:\n","            a_tran = a_tran[:-1]\n","        a_tran_final, ans_start_final, align_score = get_answer_start(index_c_tran_with_ans, a_tran, c_tran_sent_list,debug=False)\n","        \n","        \n","        if regexp.search(a_tran) or align_score<0.6: # or  \"'\" in a_tran_final or (a_tran_final[0] in align_punc or a_tran_final[-1] in align_punc):\n","            c_tran_with_ans_token = tok.utokenize_string(c_tran_sent_list[index_c_tran_with_ans]).split(\" \")\n","            a_tran_token = tok.utokenize_string(a_tran).split(\" \")\n","\n","            ans_start_final2 ,align_score2 = get_back_trans_answer_start(c_back_tran_tokens[c_tran_tokens_len[ij-1]:c_tran_tokens_len[ij]], ans_back_tran_tokens[a_tran_tokens_len[ij-1]:a_tran_tokens_len[ij]], debug=False)\n","            ij+=1\n","            a_tran_final2 = detok.detokenize_string(\" \".join(c_tran_with_ans_token[ans_start_final2:ans_start_final2+len(a_tran_token)]))\n","            a_tran_final2 = re.sub(\"([০-৯]+)([অ-য়]+)\",r\"\\1 \\2\", a_tran_final2)        \n","            ans_start_ref = c_tran_sent_list[index_c_tran_with_ans].find(a_tran_final2)\n","            ans_start_final2 = 0\n","            for i in range(len(c_tran_sent_list)):\n","                c_tran_sent = c_tran_sent_list[i]\n","                if i == index_c_tran_with_ans:\n","                    ans_start_final2+= ans_start_ref  \n","                if i < index_c_tran_with_ans:        \n","                    ans_start_final2 += len(c_tran_sent)+1\n","\n","            if align_score2>align_score+0.05:\n","                align_score = align_score2\n","                ans_start_final = ans_start_final2\n","                a_tran_final = a_tran_final2\n","\n","        ans['a_tran_temp'] = a_tran\n","        ans['a_tran'] = a_tran_final\n","        ans['a_tran_start'] = ans_start_final\n","        ans['align_score'] = float(align_score)"]},{"cell_type":"code","execution_count":null,"id":"6f38ddd1","metadata":{"id":"6f38ddd1"},"outputs":[],"source":["final_out_file = open('./translated_squad/squad2_translated_final_aligned.json', \"w\")\n","json.dump(data, final_out_file, indent = 4) # save whole data replace parts later\n","final_out_file.close()"]},{"cell_type":"code","execution_count":null,"id":"8d0419a7","metadata":{"id":"8d0419a7"},"outputs":[],"source":["for di in range(400,409):\n","    d = tran_data['data'][di]\n","    for p in d['paragraphs']:\n","        #print('english context -> ', p['context'])\n","        for qas in p['qas']:\n","            for ans in qas['answers']:\n","                if ans['align_score']<0.6:\n","                    print('bangla context with ans -> ', p['bangla_context_list'][ans['index_c_tran_with_ans']])\n","                    print('banga question -> ',qas['q_tran'])\n","                    print('align_score -> ', ans['align_score'])\n","                    print('final bangla answer span -> ', ans['a_tran'])\n","                    print('english answer -> ', ans['text'])"]},{"cell_type":"code","execution_count":null,"id":"e89393a6","metadata":{"id":"e89393a6"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install transformers==\"4.25.1\" sentencepiece==\"0.1.97\" utoken==\"0.1.8\" nltk==\"3.8.1\" datasets==\"2.8.0\" torch==\"1.13.1+cu116\" numpy==\"1.21.6\" tqdm==\"4.64.1\" --quiet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M5reWUStkCbu","outputId":"d1b636a4-047b-4568-ddb6-936fda582788","executionInfo":{"status":"ok","timestamp":1673784244902,"user_tz":-360,"elapsed":12026,"user":{"displayName":"Md. Shihab Shahriar, Lecturer, CSE","userId":"15175570008440682450"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m5.0/5.8 MB\u001b[0m \u001b[31m152.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m144.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 KB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import json\n","from pathlib import Path\n","import torch\n","from torch.utils.data import DataLoader\n","import time\n","import numpy as np"],"metadata":{"id":"62MPAUyvj_K4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from utoken import utokenize\n","from utoken import detokenize\n","tok = utokenize.Tokenizer(lang_code='ben')\n","detok = detokenize.Detokenizer(lang_code='ben')"],"metadata":{"id":"WjJ308tQ2WgM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install utoken --quiet\n","import re\n","from collections import Counter\n","from utoken import utokenize\n","from utoken import detokenize\n","tok = utokenize.Tokenizer(lang_code='ben')\n","detok = detokenize.Detokenizer(lang_code='ben')\n","\n","def repeating_prob(context):\n","    context_tok = tok.utokenize_string(context.translate(str.maketrans('', '', string.punctuation))).split(\" \")\n","    tok_count =  [context_tok.count(i) for i in context_tok]\n","    rep_token =  context_tok[tok_count.index(max(tok_count))]\n","    indices_obj = re.finditer(rep_token,context)\n","    indices = [index.start() for index in indices_obj]\n","    indices_diff = [t - s for s, t in zip(indices, indices[1:])]\n","    if len(indices_diff)==0:\n","        return 0\n","    indices_diff_count = [indices_diff.count(i) for i in indices_diff]\n","    return max(indices_diff_count)\n","\n","\n","def repeating_char_prob(context):\n","    context_tok = tok.utokenize_string(context.translate(str.maketrans('', '', string.punctuation))).split(\" \")\n","    max_char_in_each_tok = [Counter(tok).most_common(1)[0][1] for tok in context_tok]\n","    return max(max_char_in_each_tok)"],"metadata":{"id":"30ShNyt_Zcw6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = Path('./translated_squad/squad2_translated_final_aligned.json')\n","\n","# Open .json file\n","with open(path, 'rb') as f:\n","    squad_dict = json.load(f)\n","\n","train = {}\n","train['data'] = []\n","\n","# Search for each passage, its question and its answer\n","for gi in range(0,400):\n","    group = squad_dict['data'][gi]\n","    for passage in group['paragraphs']:\n","        context = passage['bangla_context']\n","        train['data'].append(context)"],"metadata":{"id":"3kCX7q87yMJU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_leng = [len(data) for data in train['data']]\n","    "],"metadata":{"id":"HLOGcOGC3oux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import statistics"],"metadata":{"id":"6wzvA03K4FCS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["statistics.quantiles(train_leng)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WiCsQGui4JCB","executionInfo":{"status":"ok","timestamp":1673191647107,"user_tz":-360,"elapsed":4,"user":{"displayName":"Md. Shihab Shahriar, Lecturer, CSE","userId":"15175570008440682450"}},"outputId":"449af5e2-5127-49a6-ea83-3df9074bcff3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[520.0, 649.0, 850.0]"]},"metadata":{},"execution_count":248}]},{"cell_type":"code","source":["repeating_char_prob(\"বোমা হামলার জন্য একটি বোমা হামলার জন্য একটি বোমা হামলার জন্য\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ikO_SHI7agQS","executionInfo":{"status":"ok","timestamp":1673100119134,"user_tz":-360,"elapsed":380,"user":{"displayName":"Md. Shihab Shahriar, Lecturer, CSE","userId":"15175570008440682450"}},"outputId":"8efe826a-3f16-4d62-e223-483097cb6dee"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["path = Path('./translated_squad/squad1_translated_final_aligned.json')\n","\n","# Open .json file\n","with open(path, 'rb') as f:\n","    squad_dict = json.load(f)\n","\n","train = {}\n","train['data'] = []\n","\n","# Search for each passage, its question and its answer\n","for gi in range(0,400):\n","    group = squad_dict['data'][gi]\n","    for passage in group['paragraphs']:\n","        context = passage['bangla_context']\n","        if repeating_prob(context)>3 or repeating_char_prob(context)>10:\n","            continue\n","        for qa in passage['qas']:\n","            data = {}\n","            data['title'] = group['title']\n","            data['context'] = context\n","            qid = qa['id']\n","            question = qa['q_tran'].strip() # question\n","            data['question'] = question\n","            data['id'] = qid\n","            if repeating_prob(question)>3 or repeating_char_prob(question)>10:\n","                continue\n","            answer_start = []\n","            answer_text = []\n","            if len(qa['answers'])==0:\n","                print(data)\n","                print(qa['answers'])\n","            for answer in qa['answers']:\n","                if answer['align_score'] >= 0.5 and answer['a_tran'] in context:\n","                    answer_start.append(answer['a_tran_start'])\n","                    answer_text.append(answer['a_tran'])\n","            if len(answer_text)!=0:\n","              data['answers'] = {'answer_start':answer_start,'text':answer_text}\n","              train['data'].append(data)\n","\n","\n","out_file = open('./train_sq1.json', \"w\")\n","json.dump(train, out_file, indent = 4) # save whole data replace parts later\n","out_file.close()"],"metadata":{"id":"f6l_6QIvjXao"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = Path('./translated_squad/squad1_translated_final_aligned.json')\n","\n","# Open .json file\n","with open(path, 'rb') as f:\n","    squad_dict = json.load(f)\n","\n","val = {}\n","val['data'] = []\n","\n","# Search for each passage, its question and its answer\n","for gi in range(400,442):\n","    group = squad_dict['data'][gi]\n","    for passage in group['paragraphs']:\n","        context = passage['bangla_context']\n","        if repeating_prob(context)>3 or repeating_char_prob(context)>10:\n","            continue\n","        for qa in passage['qas']:\n","            data = {}\n","            data['title'] = group['title']\n","            data['context'] = context\n","            qid = qa['id']\n","            question = qa['q_tran'].strip() # question\n","            data['question'] = question\n","            data['id'] = qid\n","            if repeating_prob(question)>3 or repeating_char_prob(question)>10:\n","                continue\n","            answer_start = []\n","            answer_text = []\n","            for answer in qa['answers']:\n","                if answer['align_score'] >= 0.5 and answer['a_tran'] in context:\n","                    answer_start.append(answer['a_tran_start'])\n","                    answer_text.append(answer['a_tran'])\n","            if len(answer_text)!=0:\n","              data['answers'] = {'answer_start':answer_start,'text':answer_text}\n","              val['data'].append(data)\n","\n","\n","out_file = open('./val_sq1.json', \"w\")\n","json.dump(val, out_file, indent = 4) # save whole data replace parts later\n","out_file.close()"],"metadata":{"id":"LifY5catyhlG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = Path('./translated_squad/squad1_dev_translated_final_aligned.json')\n","\n","# Open .json file\n","with open(path, 'rb') as f:\n","    squad_dict = json.load(f)\n","\n","test = {}\n","test['data'] = []\n","\n","# Search for each passage, its question and its answer\n","for gi in range(len(squad_dict['data'])):\n","    group = squad_dict['data'][gi]\n","    for passage in group['paragraphs']:\n","        context = passage['bangla_context']\n","        if repeating_prob(context)>3 or repeating_char_prob(context)>10:\n","            continue\n","        for qa in passage['qas']:\n","            data = {}\n","            data['title'] = group['title']\n","            data['context'] = context\n","            qid = qa['id']\n","            question = qa['q_tran'].strip() # question\n","            data['question'] = question\n","            data['id'] = qid\n","            if repeating_prob(question)>3 or repeating_char_prob(question)>10:\n","                continue\n","            answer_start = []\n","            answer_text = []\n","            for answer in qa['answers']:\n","                if answer['align_score'] >= 0.5 and answer['a_tran'] in context:\n","                    answer_start.append(answer['a_tran_start'])\n","                    answer_text.append(answer['a_tran'])\n","            if len(answer_text)!=0:\n","              data['answers'] = {'answer_start':answer_start,'text':answer_text}\n","              test['data'].append(data)\n","\n","out_file = open('./test_sq1.json', \"w\")\n","json.dump(test, out_file, indent = 4) # save whole data replace parts later\n","out_file.close()"],"metadata":{"id":"j4C0gufowAJT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = Path('./translated_squad/squad2_translated_final_aligned.json')\n","\n","# Open .json file\n","with open(path, 'rb') as f:\n","    squad_dict = json.load(f)\n","\n","train = {}\n","train['data'] = []\n","\n","# Search for each passage, its question and its answer\n","for gi in range(0,400):\n","    group = squad_dict['data'][gi]\n","    for passage in group['paragraphs']:\n","        context = passage['bangla_context']\n","        if repeating_prob(context)>3 or repeating_char_prob(context)>10:\n","            continue\n","        for qa in passage['qas']:\n","            data = {}\n","            data['title'] = group['title']\n","            data['context'] = context\n","            qid = qa['id']\n","            question = qa['q_tran'].strip() # question\n","            data['question'] = question\n","            data['id'] = qid\n","            if repeating_prob(question)>3 or repeating_char_prob(question)>10:\n","                continue\n","            answer_start = []\n","            answer_text = []\n","            if len(qa['answers'])==0:\n","                print(data)\n","                print(qa['answers'])\n","            for answer in qa['answers']:\n","                if answer['align_score'] >= 0.5 and answer['a_tran'] in context:\n","                    answer_start.append(answer['a_tran_start'])\n","                    answer_text.append(answer['a_tran'])\n","\n","            data['answers'] = {'answer_start':answer_start,'text':answer_text}\n","            train['data'].append(data)\n","\n","\n","out_file = open('./train_sq2.json', \"w\")\n","json.dump(train, out_file, indent = 4) # save whole data replace parts later\n","out_file.close()"],"metadata":{"id":"9hk-rGS-iLKA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = Path('./translated_squad/squad2_translated_final_aligned.json')\n","\n","# Open .json file\n","with open(path, 'rb') as f:\n","    squad_dict = json.load(f)\n","\n","val = {}\n","val['data'] = []\n","\n","# Search for each passage, its question and its answer\n","for gi in range(400,442):\n","    group = squad_dict['data'][gi]\n","    for passage in group['paragraphs']:\n","        context = passage['bangla_context']\n","        if repeating_prob(context)>3 or repeating_char_prob(context)>10:\n","            continue\n","        for qa in passage['qas']:\n","            data = {}\n","            data['title'] = group['title']\n","            data['context'] = context\n","            qid = qa['id']\n","            question = qa['q_tran'].strip() # question\n","            data['question'] = question\n","            data['id'] = qid\n","            if repeating_prob(question)>3 or repeating_char_prob(question)>10:\n","                continue\n","            answer_start = []\n","            answer_text = []\n","            for answer in qa['answers']:\n","                if answer['align_score'] >= 0.5 and answer['a_tran'] in context:\n","                    answer_start.append(answer['a_tran_start'])\n","                    answer_text.append(answer['a_tran'])\n","\n","            data['answers'] = {'answer_start':answer_start,'text':answer_text}\n","            val['data'].append(data)\n","\n","\n","out_file = open('./val_sq2.json', \"w\")\n","json.dump(val, out_file, indent = 4) # save whole data replace parts later\n","out_file.close()"],"metadata":{"id":"SU6qvUCViLKB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = Path('./translated_squad/squad2_dev_translated_final_aligned.json')\n","\n","# Open .json file\n","with open(path, 'rb') as f:\n","    squad_dict = json.load(f)\n","\n","test = {}\n","test['data'] = []\n","\n","# Search for each passage, its question and its answer\n","for gi in range(len(squad_dict['data'])):\n","    group = squad_dict['data'][gi]\n","    for passage in group['paragraphs']:\n","        context = passage['bangla_context']\n","        if repeating_prob(context)>3 or repeating_char_prob(context)>10:\n","            continue\n","        for qa in passage['qas']:\n","            data = {}\n","            data['title'] = group['title']\n","            data['context'] = context\n","            qid = qa['id']\n","            question = qa['q_tran'].strip() # question\n","            data['question'] = question\n","            data['id'] = qid\n","            if repeating_prob(question)>3 or repeating_char_prob(question)>10:\n","                continue\n","            answer_start = []\n","            answer_text = []\n","            for answer in qa['answers']:\n","                if answer['align_score'] >= 0.5 and answer['a_tran'] in context:\n","                    answer_start.append(answer['a_tran_start'])\n","                    answer_text.append(answer['a_tran'])\n","\n","            data['answers'] = {'answer_start':answer_start,'text':answer_text}\n","            test['data'].append(data)\n","\n","out_file = open('./test_sq2.json', \"w\")\n","json.dump(test, out_file, indent = 4) # save whole data replace parts later\n","out_file.close()"],"metadata":{"id":"2MEkjSyqiLKB"},"execution_count":null,"outputs":[]}]}